{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LogisticRegression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOSgP6qF4gWAbsqmvUYDi0z"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iV82BpcP6U7",
        "outputId": "1dddd1eb-cfe1-4c7c-d091-fc0762681c93"
      },
      "source": [
        "# curve fit - third degree\n",
        "import numpy as np\n",
        "x = [1, 2, 3, 4, 5]\n",
        "y = [i ** 3 + 1 for i in x]\n",
        "features = [[1] + [i ** j for j in range(1, 4)] for i in x]\n",
        "features, labels = np.array(features), np.array(y)\n",
        "theta = np.array([0, 0, 0, 0])\n",
        "\n",
        "def hypothesis_(features, weights):\n",
        "    return np.matmul(features, weights)\n",
        "def cost_(hypothesis, labels):\n",
        "    return 1/2 * ((hypothesis - labels) ** 2).sum()\n",
        "def derivative(features, hypothesis, labels):\n",
        "    return np.matmul(np.transpose(features), (hypothesis - labels))\n",
        "\n",
        "EPOCHS = 100000\n",
        "learning_rate = 0.00005\n",
        "for step in range(EPOCHS + 1):\n",
        "    hypothesis = hypothesis_(features, theta)\n",
        "    delta = learning_rate * derivative(features, hypothesis, labels)\n",
        "    theta = theta - delta\n",
        "    cost = cost_(hypothesis, labels)\n",
        "    if step % 10000 == 0:\n",
        "        print(\"{:6} : cost {:10.4f}\".format(step, cost), theta)\n",
        "test_val = 127\n",
        "x_test = np.array([1] + [test_val ** j for j in range(1, 4)])\n",
        "print(np.matmul(x_test, theta))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     0 : cost 10485.0000 [0.0115 0.0497 0.224  1.037 ]\n",
            " 10000 : cost     0.1350 [0.28231799 0.20141375 0.08552008 0.97981144]\n",
            " 20000 : cost     0.0510 [ 0.44256135  0.28344872 -0.00687424  0.99406892]\n",
            " 30000 : cost     0.0200 [ 0.54063276  0.33206917 -0.06250554  1.00267296]\n",
            " 40000 : cost     0.0086 [ 0.6009836   0.36042798 -0.09583878  1.00784784]\n",
            " 50000 : cost     0.0044 [ 0.6384465   0.37650382 -0.11564864  1.01094286]\n",
            " 60000 : cost     0.0029 [ 0.66201918  0.38513662 -0.1272579   1.01277652]\n",
            " 70000 : cost     0.0023 [ 0.67715985  0.38926229 -0.13389547  1.01384534]\n",
            " 80000 : cost     0.0020 [ 0.68717957  0.39066175 -0.13751999  1.01445049]\n",
            " 90000 : cost     0.0019 [ 0.69408692  0.39041526 -0.1393196   1.01477469]\n",
            "100000 : cost     0.0019 [ 0.69910039  0.38917809 -0.14001503  1.01492875]\n",
            "2076754.6278873093\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrAYuczvP86D",
        "outputId": "d7c2980b-72fb-460e-9ca3-dae243c6fba0"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.array([[1, 1, 2],\n",
        "     [1, 2, 3],\n",
        "     [1, 3, 5]])\n",
        "y = np.array([3, 5, 8])\n",
        "theta = np.random.rand(3)\n",
        "\n",
        "def hypothesis(features, weights):\n",
        "    return np.matmul(features, weights)\n",
        "def cost(hypothesis, labels):\n",
        "    return (1/2) * ((hypothesis - labels) ** 2).sum()\n",
        "def grad(features, hypothesis, labels):\n",
        "    return np.matmul(np.transpose(features), (hypothesis - labels))\n",
        "def gradient_descent(features, labels, weights, learning_rate, EPOCHS):\n",
        "    for step in range(EPOCHS + 1):\n",
        "        hypo = hypothesis(features, weights)\n",
        "        delta = grad(features, hypo, labels)\n",
        "        weights = weights - learning_rate * delta\n",
        "        loss = cost(hypo, labels)\n",
        "        if step % (EPOCHS / 10) == 0:\n",
        "            print(\"{:4} : cost {:10.4f}\".format(step, loss))\n",
        "    return weights\n",
        "def test(features, weights, answer):\n",
        "    print(\"expect: {:5.4f} | error rate: {:5.4f} %\".format(np.matmul(features, weights), np.abs(np.matmul(features, weights) - answer) / answer * 100))\n",
        "\n",
        "theta = gradient_descent(x, y, theta, 0.01, 1000)\n",
        "test([1, 10, 20], theta, 30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   0 : cost    12.8123\n",
            " 100 : cost     0.1006\n",
            " 200 : cost     0.0482\n",
            " 300 : cost     0.0232\n",
            " 400 : cost     0.0114\n",
            " 500 : cost     0.0057\n",
            " 600 : cost     0.0030\n",
            " 700 : cost     0.0017\n",
            " 800 : cost     0.0010\n",
            " 900 : cost     0.0007\n",
            "1000 : cost     0.0005\n",
            "expect: 30.2147 | error rate: 0.7158 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKWhDbYzQCCi",
        "outputId": "2e4e88c3-f367-4d00-9174-83a5f086838a"
      },
      "source": [
        "# Logistic Regression\n",
        "import numpy as np\n",
        "x = [[0, 1],\n",
        "     [1, 3],\n",
        "     [1, 4],\n",
        "     [5, 1],\n",
        "     [3, 2],\n",
        "     [4, 2]]\n",
        "y = [0, 0, 0, 1, 1, 1]\n",
        "\n",
        "class classification:\n",
        "    def __init__(self, input, output, dim, lr, epochs):\n",
        "        self.learning_rate = lr\n",
        "        self.EPOCHS = epochs\n",
        "        self.features = np.array([[1] + xs for xs in x])\n",
        "        self.labels = np.array(output)\n",
        "        self.weights = np.random.rand(dim)\n",
        "    def sigmoid(self, z):\n",
        "        denominator = 1 + np.exp(-1 * z)\n",
        "        return 1 / denominator\n",
        "    def hypothesis(self):\n",
        "        return self.sigmoid(np.matmul(self.features, self.weights))\n",
        "    def cost(self, hypothesis, labels):\n",
        "        error = np.matmul(labels, np.log(hypothesis)) + np.matmul(1 - labels, np.log(1 - hypothesis))\n",
        "        return -1 * error\n",
        "    def grad(self, features, hypothesis, labels):\n",
        "        derivative = np.matmul(np.transpose(features), hypothesis - labels)\n",
        "        return self.learning_rate * derivative\n",
        "    def logistic_regression(self):\n",
        "        for step in range(self.EPOCHS + 1):\n",
        "            hypothesis = self.hypothesis()\n",
        "            grads = self.grad(self.features, hypothesis, self.labels)\n",
        "            self.weights = self.weights - grads\n",
        "            cost = self.cost(hypothesis, self.labels)\n",
        "            if step % (self.EPOCHS / 10) == 0:\n",
        "                print(\"{:5} | cost: {:10.4f}\".format(step, cost))\n",
        "model = classification(x, y, 3, 0.01, 100)\n",
        "model.logistic_regression()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    0 | cost:     9.3974\n",
            "   10 | cost:     3.8691\n",
            "   20 | cost:     2.1395\n",
            "   30 | cost:     1.6620\n",
            "   40 | cost:     1.3957\n",
            "   50 | cost:     1.2111\n",
            "   60 | cost:     1.0732\n",
            "   70 | cost:     0.9657\n",
            "   80 | cost:     0.8791\n",
            "   90 | cost:     0.8078\n",
            "  100 | cost:     0.7478\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwHdLN1VTewh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "366e6387-3e28-4368-96a5-731b0011d9fc"
      },
      "source": [
        "# MNIST - Logistic Regression\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "(images_train, labels_train), (images_test, labels_test) = tf.keras.datasets.mnist.load_data()\n",
        "images_train = np.reshape(images_train, (60000, 784))\n",
        "\n",
        "def one_hot_encoding(array, dim):\n",
        "    one_hot = [[0] for i in range(len(array))]\n",
        "    idx = 0\n",
        "    for x in array:\n",
        "        temp = [0] * dim\n",
        "        temp[x] = 1\n",
        "        one_hot[idx] = temp\n",
        "        idx = idx + 1\n",
        "    return one_hot\n",
        "def normalize(data):\n",
        "    return (data - np.min(data)) / (np.max(data) - np.min(data)) / 100\n",
        "\n",
        "class logistic_mnist:\n",
        "    def __init__(self, input, output, lr, epochs):\n",
        "        self.learning_rate = lr\n",
        "        self.EPOCHS = epochs\n",
        "        self.features = normalize(np.array(np.reshape(input, (60000, 784))))\n",
        "        self.labels = np.array(one_hot_encoding(output, 10))\n",
        "        self.weights = np.random.rand(784, 10)\n",
        "    def sigmoid(self, z):\n",
        "        denominator = 1 + np.exp(-1 * z)\n",
        "        return 1 / denominator\n",
        "    def hypothesis(self, features):\n",
        "        return self.sigmoid(np.matmul(features, self.weights))\n",
        "    def cost(self, hypothesis, labels):\n",
        "        error = (np.multiply(labels, np.log(hypothesis))).sum() + (np.multiply(1 - labels, np.log(1 - hypothesis))).sum()\n",
        "        return -1 * error\n",
        "    def grad(self, features, hypothesis, labels):\n",
        "        derivative = np.matmul(np.transpose(features), hypothesis - labels)\n",
        "        return self.learning_rate * derivative\n",
        "    def logistic_regression(self):\n",
        "        for step in range(self.EPOCHS + 1):\n",
        "            hypothesis = self.hypothesis(self.features)\n",
        "            grads = self.grad(self.features, hypothesis, self.labels)\n",
        "            self.weights = self.weights - grads\n",
        "            cost = self.cost(hypothesis, self.labels)\n",
        "            if step % (self.EPOCHS / 10) == 0:\n",
        "                print(\"{:5} | cost: {:10.4f}\".format(step, cost))\n",
        "model = logistic_mnist(images_train, labels_train, 0.1, 1000)\n",
        "model.logistic_regression()\n",
        "#지금 cost값이 45000정도인데 batch값을 좀 줄이면 더 줄일 수 있을듯 싶다\n",
        "#normalization에서 100을 나누었는데 이게 가능한건지는 좀 더 생각해봐야함"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    0 | cost: 559299.7190\n",
            "  100 | cost: 56614.4776\n",
            "  200 | cost: 52000.6689\n",
            "  300 | cost: 49982.4802\n",
            "  400 | cost: 48778.4754\n",
            "  500 | cost: 47950.2652\n",
            "  600 | cost: 47331.1060\n",
            "  700 | cost: 46842.2587\n",
            "  800 | cost: 46441.2161\n",
            "  900 | cost: 46102.8035\n",
            " 1000 | cost: 45811.0528\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_VCJjft4lGP",
        "outputId": "9cd36e12-1e80-4565-8d45-b7ba7395b10c"
      },
      "source": [
        "\n",
        "# MNIST - Logistic Regression\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "(images_train, labels_train), (images_test, labels_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "class logistic_mnist:\n",
        "    def __init__(self, input, output, lr, epochs):\n",
        "        self.learning_rate = lr\n",
        "        self.EPOCHS = epochs\n",
        "        #self.features = self.normalize(np.array(np.reshape(input, (60000, 784))))\n",
        "        #self.labels = np.array(self.one_hot_encoding(output, 10))\n",
        "        self.features, self.labels = self.preprocess(input, output)\n",
        "        self.weights = np.random.rand(784, 10)\n",
        "    def one_hot_encoding(self, array, dim):\n",
        "        one_hot = [[0] for i in range(len(array))]\n",
        "        idx = 0\n",
        "        for x in array:\n",
        "            temp = [0] * dim\n",
        "            temp[x] = 1\n",
        "            one_hot[idx] = temp\n",
        "            idx = idx + 1\n",
        "        return one_hot\n",
        "    def normalize(self, data):\n",
        "        # 기존 MNIST data는 0~255값을 가지기에 편차가 큼 -> sigmoid값이 1이됨 -> log(1 - hypothesis)이 nan / 따라서 전체를 normalize 후 10으로 나누었는데 이게 가능?\n",
        "        # return (data - np.min(data)) / (np.max(data) - np.min(data)) / 10 <=> data / 255 / 10\n",
        "        return data / 255.0 / 10\n",
        "    def preprocess(self, features_before, labels_before):\n",
        "        features = self.normalize(np.array(np.reshape(features_before, (len(features_before), 784))))\n",
        "        labels = np.array(self.one_hot_encoding(labels_before, 10))\n",
        "        return features, labels\n",
        "    def sigmoid(self, z):\n",
        "        denominator = 1 + np.exp(-1 * z)\n",
        "        return 1 / denominator\n",
        "    def hypothesis(self, features):\n",
        "        return self.sigmoid(np.matmul(features, self.weights))\n",
        "    def cost(self, hypothesis, labels):\n",
        "        # 어차피 elementwise 곱이 필요한것이기 때문에 굳이 matmul 후에 sum 할 필요 없이 multiply를 쓰는 것이 더 낫다고 판단(onehot encoding때문에)\n",
        "        error = (np.multiply(labels, np.log(hypothesis))).sum() + (np.multiply(1 - labels, np.log(1 - hypothesis))).sum()\n",
        "        return -1 * error\n",
        "    def grad(self, features, hypothesis, labels):\n",
        "        derivative = np.matmul(np.transpose(features), hypothesis - labels)\n",
        "        return self.learning_rate * derivative\n",
        "    def logistic_regression(self, batch_size):\n",
        "        for step in range(self.EPOCHS + 1):\n",
        "            for batch in range(int(len(self.features) / batch_size)):\n",
        "                features = self.features[(batch * batch_size):((batch + 1) * batch_size)]\n",
        "                labels = self.labels[(batch * batch_size):((batch + 1) * batch_size)]\n",
        "                hypothesis = self.hypothesis(features)\n",
        "                grads = self.grad(features, hypothesis, labels)\n",
        "                self.weights = self.weights - grads\n",
        "                cost = self.cost(hypothesis, labels)\n",
        "            if step % (self.EPOCHS / 10) == 0:\n",
        "                print(\"{:5} | cost: {:10.4f}\".format(step, cost))\n",
        "    def test_accuracy(self, test_features, test_labels):\n",
        "        features = np.array(self.normalize(np.reshape(test_features, (10000, 784))))\n",
        "        labels = np.array(self.one_hot_encoding(test_labels, 10))\n",
        "        hypothesis = self.hypothesis(features)\n",
        "        total = 0\n",
        "        for idx in range(len(labels)):\n",
        "            if np.argmax(hypothesis[idx]) == np.argmax(labels[idx]):\n",
        "                total = total + 1\n",
        "        print(\"Test Accuracy: {:10.4f}%\".format(total / len(labels) * 100))\n",
        "model = logistic_mnist(images_train, labels_train, 0.4, 100)\n",
        "model.logistic_regression(batch_size=10)\n",
        "model.test_accuracy(images_test, labels_test)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    0 | cost:     4.4259\n",
            "   10 | cost:     2.8196\n",
            "   20 | cost:     2.6828\n",
            "   30 | cost:     2.6097\n",
            "   40 | cost:     2.5577\n",
            "   50 | cost:     2.5191\n",
            "   60 | cost:     2.4902\n",
            "   70 | cost:     2.4686\n",
            "   80 | cost:     2.4523\n",
            "   90 | cost:     2.4401\n",
            "  100 | cost:     2.4309\n",
            "Test Accuracy:    91.3200%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02q0tB0eaXRh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}