{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LinearRegression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPrcY8AGT5SKLZnHk7bJXSj"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yb0N95f4eCw0",
        "outputId": "199dcdda-3afe-4a4e-eea8-1070243e025f"
      },
      "source": [
        "import numpy as np\n",
        "x = [1, 2, 3, 4, 5]\n",
        "y = [i * 2 + 1 for i in x]\n",
        "\n",
        "class LinearRegression:\n",
        "    def __init__(self, features_train, labels_train):\n",
        "        self.features = self.add_bias_column(features_train)\n",
        "        self.weights = np.random.rand(len(self.features[0]))\n",
        "        self.labels = np.array(labels_train)\n",
        "    def add_bias_column(self, data):\n",
        "        # add extra column of which elements are all 1 in order to put bias in weight vector, not adding bias separately to hypothesis\n",
        "        new_data = np.array(data)\n",
        "        if new_data.ndim == 0:\n",
        "            new_data = [new_data] + [1]\n",
        "            return new_data\n",
        "        if new_data.ndim == 1:\n",
        "            new_data = new_data[:, np.newaxis]\n",
        "        bias_column = [[1] for i in range(len(data))]\n",
        "        new_data = np.append(new_data, bias_column, axis=1)\n",
        "        return new_data\n",
        "    def hypothesis(self, features):\n",
        "        return np.matmul(features, self.weights)\n",
        "    def cost(self, hypothesis, labels):\n",
        "        return ((hypothesis - labels) ** 2).sum() / 2\n",
        "    def grad(self, features, hypothesis, labels):\n",
        "        return np.matmul(np.transpose(features), (hypothesis - labels))\n",
        "    def gradient_descent(self, lr, EPOCHS):\n",
        "        for step in range(EPOCHS + 1):\n",
        "            hypothesis = self.hypothesis(self.features)\n",
        "            gradients = self.grad(self.features, hypothesis, self.labels)\n",
        "            self.weights = self.weights - lr * gradients\n",
        "            cost = self.cost(hypothesis, self.labels)\n",
        "            if step % (EPOCHS / 10) == 0:\n",
        "                print(\"{:6} : cost {:10.4f}\".format(step, cost))\n",
        "    def train(self, lr, EPOCHS):\n",
        "        self.gradient_descent(lr, EPOCHS)\n",
        "        print(\"Training is complete.\")\n",
        "    def test(self, features_test, labels_test):\n",
        "        features = self.add_bias_column(features_test)\n",
        "        hypothesis = self.hypothesis(features)\n",
        "        print(\"Estimate Value: {:10.4f} | Answer: {} | Test Error Rate: {:10.4f}%\".format(hypothesis, labels_test, np.abs(labels_test - hypothesis) / labels_test * 100))\n",
        "model = LinearRegression(x, y)\n",
        "model.train(0.01, 1000)\n",
        "x_test = 327\n",
        "model.test(x_test, x_test * 2 + 1)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     0 : cost    49.3960\n",
            "   100 : cost     0.0090\n",
            "   200 : cost     0.0017\n",
            "   300 : cost     0.0003\n",
            "   400 : cost     0.0001\n",
            "   500 : cost     0.0000\n",
            "   600 : cost     0.0000\n",
            "   700 : cost     0.0000\n",
            "   800 : cost     0.0000\n",
            "   900 : cost     0.0000\n",
            "  1000 : cost     0.0000\n",
            "Training is complete.\n",
            "Estimate Value:   655.0060 | Answer: 655 | Test Error Rate:     0.0009%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jaEX7Wg8qDI",
        "outputId": "6ee544dc-b065-444e-b5ac-f02c5dcf2fd5"
      },
      "source": [
        "# curve fit\n",
        "import numpy as np\n",
        "x = [1, 2, 3, 4, 5]\n",
        "y = [i ** 2 + 1 for i in x]\n",
        "\n",
        "def nth_poly(vector, degree):\n",
        "    data = [[x ** i for i in range(1, degree + 1)] for x in vector]\n",
        "    return data\n",
        "\n",
        "x = nth_poly(x, 2)\n",
        "\n",
        "class LinearRegression:\n",
        "    def __init__(self, features_train, labels_train):\n",
        "        self.features = self.add_bias_column(features_train)\n",
        "        self.weights = np.random.rand(len(self.features[0]))\n",
        "        self.labels = np.array(labels_train)\n",
        "    def add_bias_column(self, data):\n",
        "        # add extra column of which elements are all 1 in order to put bias in weight vector, not adding bias separately to hypothesis\n",
        "        new_data = np.array(data)\n",
        "        if new_data.ndim == 0:\n",
        "            new_data = [new_data] + [1]\n",
        "            return new_data\n",
        "        if new_data.ndim == 1:\n",
        "            new_data = new_data[:, np.newaxis]\n",
        "        bias_column = [[1] for i in range(len(data))]\n",
        "        new_data = np.append(new_data, bias_column, axis=1)\n",
        "        return new_data\n",
        "    def hypothesis(self, features):\n",
        "        return np.matmul(features, self.weights)\n",
        "    def cost(self, hypothesis, labels):\n",
        "        return ((hypothesis - labels) ** 2).sum() / 2\n",
        "    def grad(self, features, hypothesis, labels):\n",
        "        return np.matmul(np.transpose(features), (hypothesis - labels))\n",
        "    def gradient_descent(self, lr, EPOCHS):\n",
        "        for step in range(EPOCHS + 1):\n",
        "            hypothesis = self.hypothesis(self.features)\n",
        "            gradients = self.grad(self.features, hypothesis, self.labels)\n",
        "            self.weights = self.weights - lr * gradients\n",
        "            cost = self.cost(hypothesis, self.labels)\n",
        "            if step % (EPOCHS / 10) == 0:\n",
        "                print(\"{:6} : cost {:10.4f}\".format(step, cost))\n",
        "    def train(self, lr, EPOCHS):\n",
        "        self.gradient_descent(lr, EPOCHS)\n",
        "        print(\"Training is complete.\")\n",
        "    def test(self, features_test, labels_test):\n",
        "        features = self.add_bias_column(features_test)\n",
        "        hypothesis = self.hypothesis(features)\n",
        "        print(\"Estimate Value: {:10.4f} | Answer: {} | Test Error Rate: {:10.4f}%\".format(hypothesis[0], labels_test, (np.abs(labels_test - hypothesis) / labels_test * 100)[0]))\n",
        "model = LinearRegression(x, y)\n",
        "model.train(0.001, 10000)\n",
        "x_test = 327\n",
        "features_test = nth_poly([x_test], 2)\n",
        "model.test(features_test, x_test ** 2 + 1)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     0 : cost    10.8076\n",
            "  1000 : cost     0.0547\n",
            "  2000 : cost     0.0413\n",
            "  3000 : cost     0.0312\n",
            "  4000 : cost     0.0236\n",
            "  5000 : cost     0.0178\n",
            "  6000 : cost     0.0135\n",
            "  7000 : cost     0.0102\n",
            "  8000 : cost     0.0077\n",
            "  9000 : cost     0.0058\n",
            " 10000 : cost     0.0044\n",
            "Training is complete.\n",
            "Estimate Value: 104501.4363 | Answer: 106930 | Test Error Rate:     2.2712%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXN3CZI1_cNP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4b456af-cf53-464a-d204-294bfe5feb32"
      },
      "source": [
        "# Normal Equation\n",
        "import numpy as np\n",
        "x = [1, 2, 3, 4, 5]\n",
        "y = [i ** 2 * 127 + i * 256 + 324 for i in x]\n",
        "\n",
        "def nth_poly(vector, degree):\n",
        "    data = [[x ** i for i in range(1, degree + 1)] for x in vector]\n",
        "    return data\n",
        "def add_bias_column(data):\n",
        "    # add extra column of which elements are all 1 in order to put bias in weight vector, not adding bias separately to hypothesis\n",
        "    new_data = np.array(data)\n",
        "    bias_column = [[1] for i in range(len(data))]\n",
        "    new_data = np.append(new_data, bias_column, axis=1)\n",
        "    return new_data\n",
        "\n",
        "x = add_bias_column(nth_poly(x, 2))\n",
        "print(x)\n",
        "theta = np.matmul(np.linalg.inv(np.matmul(np.transpose(x), x)), np.matmul(np.transpose(x), y))\n",
        "print(theta)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1  1  1]\n",
            " [ 2  4  1]\n",
            " [ 3  9  1]\n",
            " [ 4 16  1]\n",
            " [ 5 25  1]]\n",
            "[256. 127. 324.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TSrmiABfWas"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}