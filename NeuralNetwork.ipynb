{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralNetwork.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOxbYpQaFnu/M/8+NQzI1zf"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGukCqF3roza",
        "outputId": "2a2ff13e-1609-413e-f6c4-4b485b8a9487"
      },
      "source": [
        "# MNIST - Neural Network\n",
        "# layer 별로 weight가 있는데 이걸 각각 gradient를 구해야하는지 gradient 계산이 애매\n",
        "# 각 layer의 weight들을 하나로 묶어서 계산은 안되나도 고민\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "(images_train, labels_train), (images_test, labels_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, features_train, labels_train):\n",
        "        self.features, self.labels = self.preprocess(features_train, labels_train)\n",
        "        self.weights = np.random.rand(len(features_train[0]))\n",
        "    def preprocess(self, features_before, labels_before):\n",
        "        features = np.reshape(features_before, (len(features_before), 784))\n",
        "        features = self.normalize(features)\n",
        "        labels = self.one_hot_encoding(labels_before)\n",
        "        return np.array(features), np.array(labels)\n",
        "    def normalize(self, data):\n",
        "        return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
        "    def one_hot_encoding(self, data):\n",
        "        encoded = [([0] * (np.max(data) - np.min(data) + 1)) for i in range(len(data))]\n",
        "        for idx in range(len(data)):\n",
        "            encoded[idx][data[idx]] = 1\n",
        "        return encoded\n",
        "    def hypothesis(self, features):\n",
        "        \n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-1 * z))\n",
        "    def cost(self, hypothesis, labels):\n",
        "        return np.multiply(labels, np.log(hypothesis)) + np.multiply(1 - labels, np.log(1 - hypothesis))\n",
        "    def grad(self, features, hypothesis, labels):\n",
        "\n",
        "    def gradient_descent(self, learning_rate, EPOCHS, batch_size):\n",
        "        for step in range(EPOCHS + 1):\n",
        "            for batch_idx in range(len(self.features) / batch_size):\n",
        "                features = self.features[batch_idx * batch_size : (batch_idx + 1) * batch_size]\n",
        "                hypothesis = self.hypothesis(features)\n",
        "                grad = self.grad(features, hypothesis, self.labels)\n",
        "                self.weights = self.weights # -? +?\n",
        "                cost = self.cost(hypothesis, self.labels)\n",
        "            if step % (EPOCHS / 10) == 0:\n",
        "                print(\"{:6} : cost {:10.4f}\".format(step, cost))\n",
        "model = NeuralNetwork(images_train, labels_train)\n",
        "print(model.labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 ... 0 0 0]\n",
            " [1 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 1 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-W7u_QmtIlK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "996bc78e-7c61-42a3-fe2d-79bf6f893fa6"
      },
      "source": [
        "# 2 Layer NN(ReLU, Softmax) with no batch\n",
        "import sys\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "(images_train, labels_train), (images_test, labels_test) = tf.keras.datasets.mnist.load_data()\n",
        "images_train = np.reshape(images_train / 255.0, (len(images_train), len(images_train[0]) * len(images_train[0][0]))).T\n",
        "images_test = np.reshape(images_test / 255.0, (len(images_test), len(images_test[0]) * len(images_test[0][0]))).T\n",
        "\n",
        "def one_hot_encoding(data):\n",
        "    encoded = np.zeros((len(data), max(data) - min(data) + 1))\n",
        "    for idx in range(len(data)):\n",
        "        encoded[idx][data[idx]] = 1\n",
        "    return encoded.T\n",
        "\n",
        "labels_train, labels_test = one_hot_encoding(labels_train), one_hot_encoding(labels_test)\n",
        "\n",
        "class NeuralNetwork_2():\n",
        "    def __init__(self, n0, n1, n2):\n",
        "        self.weight1 = np.random.rand(n1, n0)\n",
        "        self.weight2 = np.random.rand(n2, n1)\n",
        "        self.bias1 = np.random.rand(n1)\n",
        "        self.bias2 = np.random.rand(n2)\n",
        "    def linear_hypo(self, w, x, b):\n",
        "        return (np.matmul(w, x).T + b).T\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "    def relu(self, z):\n",
        "        return np.maximum(z, 0)\n",
        "    def softmax(self, z):\n",
        "        return np.exp(z) / np.exp(z).sum(axis=0)\n",
        "    def hypothesis(self, func, lh):\n",
        "        if func == 'sigmoid':\n",
        "            return self.sigmoid(lh)\n",
        "        elif func == 'relu':\n",
        "            return self.relu(lh)\n",
        "        elif func == 'softmax':\n",
        "            return self.softmax(lh)\n",
        "        else:\n",
        "            sys.exit('Error in hypothesis: There is no {} function'.format(func))\n",
        "    def cost(self, y, hypo):\n",
        "        # cross entropy\n",
        "        sum = np.multiply(y, np.log(hypo)).sum(axis=0)\n",
        "        return -np.average(sum)\n",
        "    def train(self, features, labels, learning_rate, EPOCHS):\n",
        "        for iter in range(EPOCHS + 1):\n",
        "            # layer1: relu, layer2: softmax, cost: cross-entropy\n",
        "            z1 = self.linear_hypo(self.weight1, features, self.bias1)\n",
        "            layer1 = self.hypothesis('relu', z1)\n",
        "            z2 = self.linear_hypo(self.weight2, layer1, self.bias2)\n",
        "            layer2 = self.hypothesis('softmax', z2)\n",
        "            # gradient\n",
        "            dz2 = layer2 - labels\n",
        "            dw2 = np.matmul(dz2, layer1.T)\n",
        "            db2 = dz2.sum(axis=1)\n",
        "\n",
        "            dz1 = np.multiply(np.matmul(self.weight2.T, dz2), np.where(z1 > 0, 1, 0)) # relu gradient: if x > 0: 1 else: 0 => np.where(z1 > 0, 1, 0)\n",
        "            dw1 = np.matmul(dz1, features.T)\n",
        "            db1 = dz1.sum(axis=1)\n",
        "\n",
        "            self.weight2 = self.weight2 - dw2 * learning_rate\n",
        "            self.weight1 = self.weight1 - dw1 * learning_rate\n",
        "            self.bias2 = self.bias2 - db2 * learning_rate\n",
        "            self.bias1 = self.bias1 - db1 * learning_rate\n",
        "\n",
        "            error = self.cost(labels, layer2)\n",
        "            if iter % (EPOCHS / 10) == 0:\n",
        "                print(\"iter: {:4} error: {:10.4f}\".format(iter, error))\n",
        "model = NeuralNetwork_2(len(images_train), 5, 10)\n",
        "model.train(images_train, labels_train, 0.00001, 10)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter:    0 error:    29.8609\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: RuntimeWarning: overflow encountered in exp\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:43: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:43: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iter:    1 error:        nan\n",
            "iter:    2 error:        nan\n",
            "iter:    3 error:        nan\n",
            "iter:    4 error:        nan\n",
            "iter:    5 error:        nan\n",
            "iter:    6 error:        nan\n",
            "iter:    7 error:        nan\n",
            "iter:    8 error:        nan\n",
            "iter:    9 error:        nan\n",
            "iter:   10 error:        nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1fziF8c1j7W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecd70141-5cc8-4cb2-ef47-9ad8ba9fd6d6"
      },
      "source": [
        "# 2 Layer NN(ReLU, Softmax) with batch\n",
        "import sys\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "(images_train, labels_train), (images_test, labels_test) = tf.keras.datasets.mnist.load_data()\n",
        "images_train = np.reshape(images_train / 255.0, (len(images_train), len(images_train[0]) * len(images_train[0][0]))).T\n",
        "images_test = np.reshape(images_test / 255.0, (len(images_test), len(images_test[0]) * len(images_test[0][0]))).T\n",
        "\n",
        "def one_hot_encoding(data):\n",
        "    encoded = np.zeros((len(data), max(data) - min(data) + 1))\n",
        "    for idx in range(len(data)):\n",
        "        encoded[idx][data[idx]] = 1\n",
        "    return encoded.T\n",
        "\n",
        "labels_train, labels_test = one_hot_encoding(labels_train), one_hot_encoding(labels_test)\n",
        "\n",
        "class NeuralNetwork_2():\n",
        "    def __init__(self, n0, n1, n2):\n",
        "        self.weight1 = np.random.rand(n1, n0)\n",
        "        self.weight2 = np.random.rand(n2, n1)\n",
        "        self.bias1 = np.random.rand(n1)\n",
        "        self.bias2 = np.random.rand(n2)\n",
        "    def linear_hypo(self, w, x, b):\n",
        "        return (np.matmul(w, x).T + b).T\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "    def relu(self, z):\n",
        "        return np.maximum(z, 0)\n",
        "    def softmax(self, z):\n",
        "        return np.exp(z) / np.exp(z).sum(axis=0)\n",
        "    def hypothesis(self, func, lh):\n",
        "        if func == 'sigmoid':\n",
        "            return self.sigmoid(lh)\n",
        "        elif func == 'relu':\n",
        "            return self.relu(lh)\n",
        "        elif func == 'softmax':\n",
        "            return self.softmax(lh)\n",
        "        else:\n",
        "            sys.exit('Error in hypothesis: There is no {} function'.format(func))\n",
        "    def cost(self, y, hypo):\n",
        "        # cross entropy\n",
        "        sum = np.multiply(y, np.log(hypo)).sum(axis=0)\n",
        "        return -np.average(sum)\n",
        "    def train(self, features, labels, learning_rate, EPOCHS, batch_size):\n",
        "        for iter in range(EPOCHS + 1):\n",
        "            for batch in range(int(len(features[0]) / batch_size)):\n",
        "                features_batch = features[:, batch * batch_size : (batch + 1) * batch_size]\n",
        "                labels_batch = labels[:, batch * batch_size : (batch + 1) * batch_size]\n",
        "                # layer1: relu, layer2: softmax, cost: cross-entropy\n",
        "                z1 = self.linear_hypo(self.weight1, features_batch, self.bias1)\n",
        "                layer1 = self.hypothesis('relu', z1)\n",
        "                z2 = self.linear_hypo(self.weight2, layer1, self.bias2)\n",
        "                layer2 = self.hypothesis('softmax', z2)\n",
        "                # gradient\n",
        "                dz2 = layer2 - labels_batch\n",
        "                dw2 = np.matmul(dz2, layer1.T)\n",
        "                db2 = dz2.sum(axis=1)\n",
        "\n",
        "                dz1 = np.multiply(np.matmul(self.weight2.T, dz2), np.where(z1 > 0, 1, 0)) # relu gradient: if x > 0: 1 else: 0 => np.where(z1 > 0, 1, 0)\n",
        "                dw1 = np.matmul(dz1, features_batch.T)\n",
        "                db1 = dz1.sum(axis=1)\n",
        "\n",
        "                self.weight2 = self.weight2 - dw2 * learning_rate\n",
        "                self.weight1 = self.weight1 - dw1 * learning_rate\n",
        "                self.bias2 = self.bias2 - db2 * learning_rate\n",
        "                self.bias1 = self.bias1 - db1 * learning_rate\n",
        "\n",
        "                error = self.cost(labels_batch, layer2)\n",
        "            if iter % (EPOCHS / 10) == 0:\n",
        "                print(\"iter: {:4} error: {:10.4f}\".format(iter, error))\n",
        "    def test_accuracy(self, features, labels):\n",
        "        hypothesis = self.hypothesis('softmax', self.linear_hypo(self.weight2, self.hypothesis('relu', self.linear_hypo(self.weight1, features, self.bias1)), self.bias2))\n",
        "        prob = np.average((np.argmax(hypothesis, axis=0) == np.argmax(labels, axis=0))) * 100\n",
        "        print(\"Test Accuracy: {:10.4f}\".format(prob))\n",
        "model = NeuralNetwork_2(len(images_train), 5, 10)\n",
        "model.train(images_train, labels_train, 0.0001, 100, 10)\n",
        "model.test_accuracy(images_test, labels_test)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter:    0 error:     8.2868\n",
            "iter:   10 error:     0.7600\n",
            "iter:   20 error:     0.6516\n",
            "iter:   30 error:     0.5895\n",
            "iter:   40 error:     0.5544\n",
            "iter:   50 error:     0.5317\n",
            "iter:   60 error:     0.5156\n",
            "iter:   70 error:     0.5034\n",
            "iter:   80 error:     0.4939\n",
            "iter:   90 error:     0.4862\n",
            "iter:  100 error:     0.4799\n",
            "Test Accuracy:    87.7600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMLuJHM0V8WK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}