{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralNetwork.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPeExkwvdhgFKvqsJDDrbc7"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uv7PokoRIddX"
      },
      "source": [
        "All codes are based on \"Neural Networks and Deep Learning\" Course by Deeplearning.ai"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-W7u_QmtIlK",
        "outputId": "fcb0571b-7a62-4fcd-a9c6-fa46aaddb557"
      },
      "source": [
        "# 2 Layer NN(ReLU, Softmax) with no batch\n",
        "import sys\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "(images_train, labels_train), (images_test, labels_test) = tf.keras.datasets.mnist.load_data()\n",
        "images_train = np.reshape(images_train / 255.0, (len(images_train), len(images_train[0]) * len(images_train[0][0]))).T\n",
        "images_test = np.reshape(images_test / 255.0, (len(images_test), len(images_test[0]) * len(images_test[0][0]))).T\n",
        "\n",
        "def one_hot_encoding(data):\n",
        "    encoded = np.zeros((len(data), max(data) - min(data) + 1))\n",
        "    for idx in range(len(data)):\n",
        "        encoded[idx][data[idx]] = 1\n",
        "    return encoded.T\n",
        "\n",
        "labels_train, labels_test = one_hot_encoding(labels_train), one_hot_encoding(labels_test)\n",
        "\n",
        "class NeuralNetwork_2():\n",
        "    def __init__(self, n0, n1, n2):\n",
        "        self.weight1 = np.random.rand(n1, n0) * 0.01\n",
        "        self.weight2 = np.random.rand(n2, n1) * 0.01\n",
        "        self.bias1 = np.random.rand(n1)\n",
        "        self.bias2 = np.random.rand(n2)\n",
        "    def linear_hypo(self, w, x, b):\n",
        "        return (np.matmul(w, x).T + b).T\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "    def relu(self, z):\n",
        "        return np.maximum(z, 0)\n",
        "    def softmax(self, z):\n",
        "        return np.exp(z) / np.exp(z).sum(axis=0)\n",
        "    def hypothesis(self, func, lh):\n",
        "        if func == 'sigmoid':\n",
        "            return self.sigmoid(lh)\n",
        "        elif func == 'relu':\n",
        "            return self.relu(lh)\n",
        "        elif func == 'softmax':\n",
        "            return self.softmax(lh)\n",
        "        else:\n",
        "            sys.exit('Error in hypothesis: There is no {} function'.format(func))\n",
        "    def cost(self, y, hypo):\n",
        "        # cross entropy\n",
        "        sum = np.multiply(y, np.log(hypo)).sum(axis=0)\n",
        "        return -np.average(sum)\n",
        "    def train(self, features, labels, learning_rate, EPOCHS):\n",
        "        for iter in range(EPOCHS + 1):\n",
        "            # layer1: relu, layer2: softmax, cost: cross-entropy\n",
        "            z1 = self.linear_hypo(self.weight1, features, self.bias1)\n",
        "            layer1 = self.hypothesis('relu', z1)\n",
        "            z2 = self.linear_hypo(self.weight2, layer1, self.bias2)\n",
        "            layer2 = self.hypothesis('softmax', z2)\n",
        "            # gradient\n",
        "            dz2 = layer2 - labels\n",
        "            dw2 = np.matmul(dz2, layer1.T)\n",
        "            db2 = np.average(dz2, axis=1)\n",
        "\n",
        "            dz1 = np.multiply(np.matmul(self.weight2.T, dz2), np.where(z1 > 0, 1, 0)) # relu gradient: if x > 0: 1 else: 0 => np.where(z1 > 0, 1, 0)\n",
        "            dw1 = np.matmul(dz1, features.T)\n",
        "            db1 = np.average(dz1, axis=1)\n",
        "\n",
        "            self.weight2 = self.weight2 - dw2 * learning_rate\n",
        "            self.weight1 = self.weight1 - dw1 * learning_rate\n",
        "            self.bias2 = self.bias2 - db2 * learning_rate\n",
        "            self.bias1 = self.bias1 - db1 * learning_rate\n",
        "\n",
        "            error = self.cost(labels, layer2)\n",
        "            if iter % (EPOCHS / 10) == 0:\n",
        "                print(\"iter: {:4} error: {:10.4f}\".format(iter, error))\n",
        "model = NeuralNetwork_2(len(images_train), 5, 10)\n",
        "model.train(images_train, labels_train, 0.001, 100)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter:    0 error:     2.3499\n",
            "iter:   10 error:     2.3504\n",
            "iter:   20 error:     2.3503\n",
            "iter:   30 error:     2.3502\n",
            "iter:   40 error:     2.3501\n",
            "iter:   50 error:     2.3500\n",
            "iter:   60 error:     2.3499\n",
            "iter:   70 error:     2.3499\n",
            "iter:   80 error:     2.3498\n",
            "iter:   90 error:     2.3497\n",
            "iter:  100 error:     2.3496\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1fziF8c1j7W",
        "outputId": "f5077bd8-f417-4273-a0b8-38e86b2f15ba"
      },
      "source": [
        "# 2 Layer NN(ReLU, Softmax) with batch\n",
        "import sys\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "(images_train, labels_train), (images_test, labels_test) = tf.keras.datasets.mnist.load_data()\n",
        "images_train = np.reshape(images_train / 255.0, (len(images_train), len(images_train[0]) * len(images_train[0][0]))).T\n",
        "images_test = np.reshape(images_test / 255.0, (len(images_test), len(images_test[0]) * len(images_test[0][0]))).T\n",
        "\n",
        "def one_hot_encoding(data):\n",
        "    encoded = np.zeros((len(data), max(data) - min(data) + 1))\n",
        "    for idx in range(len(data)):\n",
        "        encoded[idx][data[idx]] = 1\n",
        "    return encoded.T\n",
        "\n",
        "labels_train, labels_test = one_hot_encoding(labels_train), one_hot_encoding(labels_test)\n",
        "\n",
        "class NeuralNetwork_2():\n",
        "    def __init__(self, n0, n1, n2):\n",
        "        self.weight1 = np.random.rand(n1, n0) * 0.01\n",
        "        self.weight2 = np.random.rand(n2, n1) * 0.01\n",
        "        self.bias1 = np.random.rand(n1)\n",
        "        self.bias2 = np.random.rand(n2)\n",
        "    def linear_hypo(self, w, x, b):\n",
        "        return (np.matmul(w, x).T + b).T\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "    def relu(self, z):\n",
        "        return np.maximum(z, 0)\n",
        "    def softmax(self, z):\n",
        "        return np.exp(z) / np.exp(z).sum(axis=0) \n",
        "    def hypothesis(self, func, lh):\n",
        "        if func == 'sigmoid':\n",
        "            return self.sigmoid(lh)\n",
        "        elif func == 'relu':\n",
        "            return self.relu(lh)\n",
        "        elif func == 'softmax':\n",
        "            return self.softmax(lh)\n",
        "        else:\n",
        "            sys.exit('Error in hypothesis: There is no {} function'.format(func))\n",
        "    def cost(self, y, hypo):\n",
        "        # cross entropy\n",
        "        sum = np.multiply(y, np.log(hypo)).sum(axis=0)\n",
        "        return -np.average(sum)\n",
        "    def train(self, features, labels, learning_rate, EPOCHS, batch_size):\n",
        "        for iter in range(EPOCHS + 1):\n",
        "            for batch in range(int(len(features[0]) / batch_size)):\n",
        "                features_batch = features[:, batch * batch_size : (batch + 1) * batch_size]\n",
        "                labels_batch = labels[:, batch * batch_size : (batch + 1) * batch_size]\n",
        "                # layer1: relu, layer2: softmax, cost: cross-entropy\n",
        "                z1 = self.linear_hypo(self.weight1, features_batch, self.bias1)\n",
        "                layer1 = self.hypothesis('relu', z1)\n",
        "                z2 = self.linear_hypo(self.weight2, layer1, self.bias2)\n",
        "                layer2 = self.hypothesis('softmax', z2)\n",
        "                # gradient\n",
        "                dz2 = layer2 - labels_batch #this is same result when you use sigmoid and sigmoid's cost function(y*log(hypo) + (1-y)*log(1-hypo))\n",
        "                dw2 = np.matmul(dz2, layer1.T)\n",
        "                db2 = np.average(dz2, axis=1)\n",
        "\n",
        "                dz1 = np.multiply(np.matmul(self.weight2.T, dz2), np.where(z1 > 0, 1, 0)) # relu gradient: if x > 0: 1 else: 0 => np.where(z1 > 0, 1, 0)\n",
        "                dw1 = np.matmul(dz1, features_batch.T)\n",
        "                db1 = np.average(dz1, axis=1)\n",
        "\n",
        "                self.weight2 = self.weight2 - dw2 * learning_rate\n",
        "                self.weight1 = self.weight1 - dw1 * learning_rate\n",
        "                self.bias2 = self.bias2 - db2 * learning_rate\n",
        "                self.bias1 = self.bias1 - db1 * learning_rate\n",
        "\n",
        "                error = self.cost(labels_batch, layer2)\n",
        "            #if iter % (EPOCHS / 10) == 0:\n",
        "            #    print(\"iter: {:4} error: {:10.4f}\".format(iter, error))\n",
        "    def test_accuracy(self, features, labels):\n",
        "        hypothesis = self.hypothesis('softmax', self.linear_hypo(self.weight2, self.hypothesis('relu', self.linear_hypo(self.weight1, features, self.bias1)), self.bias2))\n",
        "        prob = np.average((np.argmax(hypothesis, axis=0) == np.argmax(labels, axis=0))) * 100\n",
        "        print(\"Test Accuracy: {:10.4f}\".format(prob))\n",
        "model = NeuralNetwork_2(len(images_train), 100, 10)\n",
        "model.train(images_train, labels_train, 0.0001, 100, 100)\n",
        "model.test_accuracy(images_test, labels_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "Test Accuracy:    96.5800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2f_Wu31nfSx"
      },
      "source": [
        "#Find best setting of model evaluated by Test Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6JPTx7NKl6u",
        "outputId": "d500493c-701d-475e-8c8c-173c6ed6da8b"
      },
      "source": [
        "# Test Accuracy comparison with changing learning_rate - 0.0001 is the best\n",
        "model = NeuralNetwork_2(len(images_train), 5, 10)\n",
        "model.train(images_train, labels_train, 0.1, 100, 50)\n",
        "model.test_accuracy(images_test, labels_test)\n",
        "\n",
        "model = NeuralNetwork_2(len(images_train), 5, 10)\n",
        "model.train(images_train, labels_train, 0.01, 100, 50)\n",
        "model.test_accuracy(images_test, labels_test)\n",
        "\n",
        "model = NeuralNetwork_2(len(images_train), 5, 10)\n",
        "model.train(images_train, labels_train, 0.001, 100, 50)\n",
        "model.test_accuracy(images_test, labels_test)\n",
        "\n",
        "model = NeuralNetwork_2(len(images_train), 5, 10)\n",
        "model.train(images_train, labels_train, 0.0001, 100, 50)\n",
        "model.test_accuracy(images_test, labels_test)\n",
        "\n",
        "model = NeuralNetwork_2(len(images_train), 5, 10)\n",
        "model.train(images_train, labels_train, 0.00001, 100, 50)\n",
        "model.test_accuracy(images_test, labels_test)\n",
        "\n",
        "model = NeuralNetwork_2(len(images_train), 5, 10)\n",
        "model.train(images_train, labels_train, 0.000001, 100, 50)\n",
        "model.test_accuracy(images_test, labels_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy:    10.2800\n",
            "Test Accuracy:    86.4100\n",
            "Test Accuracy:    88.7300\n",
            "Test Accuracy:    89.4500\n",
            "Test Accuracy:    86.4200\n",
            "Test Accuracy:    12.1900\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dkAnpq0NRYX",
        "outputId": "5ad63212-ec95-4030-8bd5-69fa05a01915"
      },
      "source": [
        "# Test Accuracy comparison with changing batch_size - 100 is the best\n",
        "model = NeuralNetwork_2(len(images_train), 5, 10)\n",
        "model.train(images_train, labels_train, 0.0001, 100, 10)\n",
        "model.test_accuracy(images_test, labels_test)\n",
        "\n",
        "model = NeuralNetwork_2(len(images_train), 5, 10)\n",
        "model.train(images_train, labels_train, 0.0001, 100, 50)\n",
        "model.test_accuracy(images_test, labels_test)\n",
        "\n",
        "model = NeuralNetwork_2(len(images_train), 5, 10)\n",
        "model.train(images_train, labels_train, 0.0001, 100, 100)\n",
        "model.test_accuracy(images_test, labels_test)\n",
        "\n",
        "model = NeuralNetwork_2(len(images_train), 5, 10)\n",
        "model.train(images_train, labels_train, 0.0001, 100, 1000)\n",
        "model.test_accuracy(images_test, labels_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy:    89.5300\n",
            "Test Accuracy:    89.5200\n",
            "Test Accuracy:    89.7100\n",
            "Test Accuracy:    89.6400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXQ3YnrLOQut",
        "outputId": "50db9c49-fae4-4b5c-fa30-3592bad430a9"
      },
      "source": [
        "# Test Accuracy comparison with changing number of hidden layer nodes - 100 is the best\n",
        "model = NeuralNetwork_2(len(images_train), 20, 10)\n",
        "model.train(images_train, labels_train, 0.0001, 100, 100)\n",
        "model.test_accuracy(images_test, labels_test)\n",
        "\n",
        "model = NeuralNetwork_2(len(images_train), 50, 10)\n",
        "model.train(images_train, labels_train, 0.0001, 100, 100)\n",
        "model.test_accuracy(images_test, labels_test)\n",
        "\n",
        "model = NeuralNetwork_2(len(images_train), 100, 10)\n",
        "model.train(images_train, labels_train, 0.0001, 100, 100)\n",
        "model.test_accuracy(images_test, labels_test)\n",
        "\n",
        "model = NeuralNetwork_2(len(images_train), 1000, 10) # too much time needed to calculate\n",
        "model.train(images_train, labels_train, 0.0001, 100, 100)\n",
        "model.test_accuracy(images_test, labels_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy:    95.2200\n",
            "Test Accuracy:    95.9300\n",
            "Test Accuracy:    96.3200\n",
            "Test Accuracy:    95.4800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iof00sGOYJ6l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21bf8bdc-7917-4bdc-811b-9d5a8fb8349a"
      },
      "source": [
        "# Test Accuracy comparison with changing EPOCHS - as many as possible\n",
        "# 100 nodes of hidden layer's calculation takes too much time so decreased it to 20\n",
        "model = NeuralNetwork_2(len(images_train), 20, 10)\n",
        "model.train(images_train, labels_train, 0.0001, 10, 100)\n",
        "model.test_accuracy(images_test, labels_test)\n",
        "\n",
        "model = NeuralNetwork_2(len(images_train), 20, 10)\n",
        "model.train(images_train, labels_train, 0.0001, 100, 100)\n",
        "model.test_accuracy(images_test, labels_test)\n",
        "\n",
        "model = NeuralNetwork_2(len(images_train), 20, 10)\n",
        "model.train(images_train, labels_train, 0.0001, 1000, 100)\n",
        "model.test_accuracy(images_test, labels_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy:    90.9200\n",
            "Test Accuracy:    94.8000\n",
            "Test Accuracy:    95.8800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMLuJHM0V8WK",
        "outputId": "dd511319-80c3-423e-bc54-9cc45ae2bfae"
      },
      "source": [
        "# 3 Layer NN(ReLU, Softmax) with no batch\n",
        "import sys\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "(images_train, labels_train), (images_test, labels_test) = tf.keras.datasets.mnist.load_data()\n",
        "images_train = np.reshape(images_train / 255.0, (len(images_train), len(images_train[0]) * len(images_train[0][0]))).T\n",
        "images_test = np.reshape(images_test / 255.0, (len(images_test), len(images_test[0]) * len(images_test[0][0]))).T\n",
        "\n",
        "def one_hot_encoding(data):\n",
        "    encoded = np.zeros((len(data), max(data) - min(data) + 1))\n",
        "    for idx in range(len(data)):\n",
        "        encoded[idx][data[idx]] = 1\n",
        "    return encoded.T\n",
        "\n",
        "labels_train, labels_test = one_hot_encoding(labels_train), one_hot_encoding(labels_test)\n",
        "\n",
        "class NeuralNetwork_3():\n",
        "    def __init__(self, n0, n1, n2, n3):\n",
        "        self.weight1 = np.random.rand(n1, n0) * 0.01\n",
        "        self.weight2 = np.random.rand(n2, n1) * 0.01\n",
        "        self.weight3 = np.random.rand(n3, n2) * 0.01\n",
        "        self.bias1 = np.random.rand(n1)\n",
        "        self.bias2 = np.random.rand(n2)\n",
        "        self.bias3 = np.random.rand(n3)\n",
        "    def linear_hypo(self, w, x, b):\n",
        "        return (np.matmul(w, x).T + b).T\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "    def relu(self, z):\n",
        "        return np.maximum(z, 0)\n",
        "    def softmax(self, z):\n",
        "        return np.exp(z) / np.exp(z).sum(axis=0)\n",
        "    def hypothesis(self, func, lh):\n",
        "        if func == 'sigmoid':\n",
        "            return self.sigmoid(lh)\n",
        "        elif func == 'relu':\n",
        "            return self.relu(lh)\n",
        "        elif func == 'softmax':\n",
        "            return self.softmax(lh)\n",
        "        else:\n",
        "            sys.exit('Error in hypothesis: There is no {} function'.format(func))\n",
        "    def cost(self, y, hypo):\n",
        "        # cross entropy\n",
        "        sum = np.multiply(y, np.log(hypo)).sum(axis=0)\n",
        "        return -np.average(sum)\n",
        "    def train(self, features, labels, learning_rate, EPOCHS, batch_size):\n",
        "        for iter in range(EPOCHS + 1):\n",
        "            for batch in range(int(len(features[0]) / batch_size)):\n",
        "                features_batch = features[:, batch * batch_size : (batch + 1) * batch_size]\n",
        "                labels_batch = labels[:, batch * batch_size : (batch + 1) * batch_size]\n",
        "                # layer1: relu, layer2: relu, layer3: softmax, cost: cross-entropy\n",
        "                z1 = self.linear_hypo(self.weight1, features_batch, self.bias1)\n",
        "                layer1 = self.hypothesis('relu', z1)\n",
        "                z2 = self.linear_hypo(self.weight2, layer1, self.bias2)\n",
        "                layer2 = self.hypothesis('relu', z2)\n",
        "                z3 = self.linear_hypo(self.weight3, layer2, self.bias3)\n",
        "                layer3 = self.hypothesis('softmax', z3)\n",
        "                # gradient\n",
        "                dz3 = layer3 - labels_batch\n",
        "                dw3 = np.matmul(dz3, layer2.T)\n",
        "                db3 = np.average(dz3, axis=1)\n",
        "\n",
        "                dz2 = np.multiply(np.matmul(self.weight3.T, dz3), np.where(z2 > 0, 1, 0))\n",
        "                dw2 = np.matmul(dz2, layer1.T)\n",
        "                db2 = np.average(dz2, axis=1)\n",
        "\n",
        "                dz1 = np.multiply(np.matmul(self.weight2.T, dz2), np.where(z1 > 0, 1, 0))\n",
        "                dw1 = np.matmul(dz1, features_batch.T)\n",
        "                db1 = np.average(dz1, axis=1)\n",
        "\n",
        "                self.weight3 = self.weight3 - dw3 * learning_rate\n",
        "                self.weight2 = self.weight2 - dw2 * learning_rate\n",
        "                self.weight1 = self.weight1 - dw1 * learning_rate\n",
        "                self.bias3 = self.bias3 - db3 * learning_rate\n",
        "                self.bias2 = self.bias2 - db2 * learning_rate\n",
        "                self.bias1 = self.bias1 - db1 * learning_rate\n",
        "\n",
        "                error = self.cost(labels_batch, layer3)\n",
        "            if iter % (EPOCHS / 10) == 0:\n",
        "                print(\"iter: {:4} error: {:10.4f}\".format(iter, error))\n",
        "    def test_accuracy(self, features, labels):\n",
        "        layer1 = self.linear_hypo(self.weight1, features, self.bias1)\n",
        "        layer1 = self.hypothesis('relu', layer1)\n",
        "        layer2 = self.linear_hypo(self.weight2, layer1, self.bias2)\n",
        "        layer2 = self.hypothesis('relu', layer2)\n",
        "        layer3 = self.linear_hypo(self.weight3, layer2, self.bias3)\n",
        "        hypothesis = self.hypothesis('softmax', layer3)\n",
        "        prob = np.average((np.argmax(hypothesis, axis=0) == np.argmax(labels, axis=0))) * 100\n",
        "        print(\"Test Accuracy: {:10.4f}\".format(prob))\n",
        "model = NeuralNetwork_3(len(images_train), 100, 100, len(labels_train))\n",
        "model.train(images_train, labels_train, 0.0001, 100, 100)\n",
        "model.test_accuracy(images_test, labels_test)"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter:    0 error:     2.2805\n",
            "iter:   10 error:     1.0050\n",
            "iter:   20 error:     0.4509\n",
            "iter:   30 error:     0.3243\n",
            "iter:   40 error:     0.2571\n",
            "iter:   50 error:     0.2117\n",
            "iter:   60 error:     0.1912\n",
            "iter:   70 error:     0.1804\n",
            "iter:   80 error:     0.1713\n",
            "iter:   90 error:     0.1640\n",
            "iter:  100 error:     0.1593\n",
            "Test Accuracy:    96.4900\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OBqi0TcTX1_"
      },
      "source": [
        "#Multi Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WONKv6Ho7MRt"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Func():\n",
        "    def sigmoid(z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "    def relu(z):\n",
        "        return np.maximum(z, 0)\n",
        "    def softmax(z): #same column: one example => sum(axis=0): sum of all values of one example and divide each classes of one example by this sum\n",
        "        return np.exp(z) / np.exp(z).sum(axis=0) #sum(axis=0) => add same column numbers and so #rows become 1\n",
        "    def softmax_modified(z):\n",
        "        #since softmax has exp and exp can make numbers too large very easily and it causes overflow in caculation\n",
        "        z = (z - z.min()) / (z.max() - z.min())\n",
        "        #print(\"z:\", z, \"softmax:\", np.exp(z) / np.exp(z).sum(axis=0))\n",
        "        return np.exp(z) / np.exp(z).sum(axis=0)\n",
        "\n",
        "class Layer():\n",
        "    def __init__(self, input_size, output_size, act_func):\n",
        "        self.weight = np.random.rand(output_size, input_size) * 0.01\n",
        "        self.bias = np.random.rand(output_size, 1) * 0.01\n",
        "        self.act_func = act_func\n",
        "        self.cache = None\n",
        "    def linear(self, input):\n",
        "        self.cache = np.matmul(self.weight, input) + self.bias #size: (n1, n0) * (n0, m) + (broadcasting)(n1, 1) => (n1, m)\n",
        "        return self.cache\n",
        "    def activate_func(self, input):\n",
        "        if self.act_func == 'sigmoid':\n",
        "            return Func.sigmoid(input)\n",
        "        elif self.act_func == 'relu':\n",
        "            return Func.relu(input)\n",
        "        elif self.act_func == 'softmax':\n",
        "            return Func.softmax_modified(input)\n",
        "    def output(self, input):\n",
        "        return self.forward(input)\n",
        "    def forward(self, input):\n",
        "        return self.activate_func(self.linear(input))\n",
        "    def backward(self, gradient_next, hypo_before, lr): #next: next layer(outputlayer if this is hidden in 3layerNN), before: before layer(inputlayer if this is hidden in 3layerNN)\n",
        "        if self.act_func == 'relu':\n",
        "            dz = np.multiply(gradient_next, np.where(self.cache > 0, 1, 0))\n",
        "        else: #sigmoid or softmax\n",
        "            dz = gradient_next\n",
        "        dw = np.matmul(dz, hypo_before.T)\n",
        "        db = np.average(dz, axis=1)\n",
        "\n",
        "        self.weight = self.weight - lr * dw\n",
        "        if np.isnan(self.weight).any():\n",
        "            print(\"BackProp Nan\")\n",
        "            print(\"dw:\", dw, \"dz\", dz, \"hypo_before\", hypo_before)\n",
        "            return None\n",
        "        self.db = self.bias - lr * db\n",
        "        return dz\n",
        "\n",
        "class NeuralNetwork():\n",
        "    def __init__(self, inputUnits, outputUnits, hiddenUnits, hiddenLayers, activation_func):\n",
        "        self.Layers = [Layer(inputUnits, hiddenUnits, activation_func)]\n",
        "        for i in range(hiddenLayers):\n",
        "            self.Layers.append(Layer(hiddenUnits, hiddenUnits, activation_func))\n",
        "        self.Layers.append(Layer(hiddenUnits, outputUnits, 'softmax'))\n",
        "\n",
        "    def cost(self, hypo, label):\n",
        "        #cross entropy\n",
        "        total = np.multiply(label, np.log(hypo)).sum(axis=0)\n",
        "        return -np.average(total)\n",
        "\n",
        "    def train(self, input_, output_, batch_size, lr, EPOCHS):\n",
        "        for iter in range(EPOCHS + 1):\n",
        "            for i in range(int(len(input_[0]) / batch_size)):\n",
        "                input = input_[:, i * batch_size : (i + 1) * batch_size]\n",
        "                output = output_[:, i * batch_size : (i + 1) * batch_size]\n",
        "                hypothesis = []\n",
        "\n",
        "                #forward propagation\n",
        "                for layer in self.Layers:\n",
        "                    if layer == self.Layers[0]:\n",
        "                        hypothesis.append(layer.forward(input))\n",
        "                    else:\n",
        "                        hypothesis.append(layer.forward(hypothesis[-1]))\n",
        "\n",
        "                #Back propagation\n",
        "                gradient = self.Layers[-1].backward(hypothesis[-1] - output, hypothesis[-2], lr)\n",
        "                l = len(hypothesis)\n",
        "                for idx in range(2, l):\n",
        "                    temp = np.matmul(self.Layers[l - idx + 1].weight.T, gradient)\n",
        "                    gradient = self.Layers[l - idx].backward(temp, hypothesis[l - idx - 1], lr)\n",
        "                temp = np.matmul(self.Layers[1].weight.T, gradient)\n",
        "                self.Layers[0].backward(temp, input, lr)\n",
        "                \n",
        "                error = self.cost(hypothesis[-1], output)\n",
        "            if iter % (EPOCHS / 10) == 0:\n",
        "                print(\"iter: {:4} error: {:10.4f}\".format(iter, error))\n",
        "\n",
        "    def test_accuracy(self, input, output):\n",
        "        for layer in self.Layers:\n",
        "            if layer == self.Layers[0]:\n",
        "                hypo = layer.forward(input)\n",
        "            else:\n",
        "                hypo = layer.forward(hypo)\n",
        "        prob = np.average((np.argmax(hypo, axis=0) == np.argmax(output, axis=0))) * 100\n",
        "        print(\"Test Accuracy: {:10.4f}\".format(prob))    \n"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQoLmRUPTkIw",
        "outputId": "636f1e18-a9b0-4b00-80c6-2ff19f21d977"
      },
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "(images_train, labels_train), (images_test, labels_test) = tf.keras.datasets.mnist.load_data()\n",
        "images_train = np.reshape(images_train / 255.0, (len(images_train), len(images_train[0]) * len(images_train[0][0]))).T\n",
        "images_test = np.reshape(images_test / 255.0, (len(images_test), len(images_test[0]) * len(images_test[0][0]))).T\n",
        "\n",
        "def one_hot_encoding(data):\n",
        "    encoded = np.zeros((len(data), max(data) - min(data) + 1))\n",
        "    for idx in range(len(data)):\n",
        "        encoded[idx][data[idx]] = 1\n",
        "    return encoded.T\n",
        "\n",
        "labels_train, labels_test = one_hot_encoding(labels_train), one_hot_encoding(labels_test)\n",
        "model = NeuralNetwork(len(images_train), len(labels_train), hiddenUnits=100, hiddenLayers=10, activation_func='relu')\n",
        "model.train(images_train, labels_train, batch_size=100, lr=0.0001, EPOCHS=100)"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter:    0 error:     2.3468\n",
            "iter:   10 error:     2.2785\n",
            "iter:   20 error:     2.2777\n",
            "iter:   30 error:     2.2768\n",
            "iter:   40 error:     2.2759\n",
            "iter:   50 error:     2.2750\n",
            "iter:   60 error:     2.2741\n",
            "iter:   70 error:     2.2732\n",
            "iter:   80 error:     2.2723\n",
            "iter:   90 error:     2.2715\n",
            "iter:  100 error:     2.2706\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8nBInn4WTYL"
      },
      "source": [
        "Test Accuracy is very low. Need to know why it is that low."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVncEXPXWh2O"
      },
      "source": [
        "Guess1. Learning rate is very low. But if I increase lr, layer's weight becomes too large and so be Inf and makes error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlSF6EqQc9Pf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0cbf9f8-cf2f-41af-e53c-d35cdd8f39b5"
      },
      "source": [
        "model.test_accuracy(images_test, labels_test)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy:    17.7100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eu-V0lUd9YEE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}