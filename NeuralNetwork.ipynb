{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "NeuralNetwork.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOaImGJaYB+aW7ZziWYOBW1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uv7PokoRIddX"
      },
      "source": [
        "All codes are based on \"Neural Networks and Deep Learning\" Course by Deeplearning.ai"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-W7u_QmtIlK",
        "outputId": "fcb0571b-7a62-4fcd-a9c6-fa46aaddb557"
      },
      "source": [
        "# 2 Layer NN(ReLU, Softmax) with no batch\n",
        "import sys\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "(images_train, labels_train), (images_test, labels_test) = tf.keras.datasets.mnist.load_data()\n",
        "images_train = np.reshape(images_train / 255.0, (len(images_train), len(images_train[0]) * len(images_train[0][0]))).T\n",
        "images_test = np.reshape(images_test / 255.0, (len(images_test), len(images_test[0]) * len(images_test[0][0]))).T\n",
        "\n",
        "def one_hot_encoding(data):\n",
        "    encoded = np.zeros((len(data), max(data) - min(data) + 1))\n",
        "    for idx in range(len(data)):\n",
        "        encoded[idx][data[idx]] = 1\n",
        "    return encoded.T\n",
        "\n",
        "labels_train, labels_test = one_hot_encoding(labels_train), one_hot_encoding(labels_test)\n",
        "\n",
        "class NeuralNetwork_2():\n",
        "    def __init__(self, n0, n1, n2):\n",
        "        self.weight1 = np.random.rand(n1, n0) * 0.01\n",
        "        self.weight2 = np.random.rand(n2, n1) * 0.01\n",
        "        self.bias1 = np.random.rand(n1)\n",
        "        self.bias2 = np.random.rand(n2)\n",
        "    def linear_hypo(self, w, x, b):\n",
        "        return (np.matmul(w, x).T + b).T\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "    def relu(self, z):\n",
        "        return np.maximum(z, 0)\n",
        "    def softmax(self, z):\n",
        "        return np.exp(z) / np.exp(z).sum(axis=0)\n",
        "    def hypothesis(self, func, lh):\n",
        "        if func == 'sigmoid':\n",
        "            return self.sigmoid(lh)\n",
        "        elif func == 'relu':\n",
        "            return self.relu(lh)\n",
        "        elif func == 'softmax':\n",
        "            return self.softmax(lh)\n",
        "        else:\n",
        "            sys.exit('Error in hypothesis: There is no {} function'.format(func))\n",
        "    def cost(self, y, hypo):\n",
        "        # cross entropy\n",
        "        sum = np.multiply(y, np.log(hypo)).sum(axis=0)\n",
        "        return -np.average(sum)\n",
        "    def train(self, features, labels, learning_rate, EPOCHS):\n",
        "        for iter in range(EPOCHS + 1):\n",
        "            # layer1: relu, layer2: softmax, cost: cross-entropy\n",
        "            z1 = self.linear_hypo(self.weight1, features, self.bias1)\n",
        "            layer1 = self.hypothesis('relu', z1)\n",
        "            z2 = self.linear_hypo(self.weight2, layer1, self.bias2)\n",
        "            layer2 = self.hypothesis('softmax', z2)\n",
        "            # gradient\n",
        "            dz2 = layer2 - labels\n",
        "            dw2 = np.matmul(dz2, layer1.T)\n",
        "            db2 = np.average(dz2, axis=1)\n",
        "\n",
        "            dz1 = np.multiply(np.matmul(self.weight2.T, dz2), np.where(z1 > 0, 1, 0)) # relu gradient: if x > 0: 1 else: 0 => np.where(z1 > 0, 1, 0)\n",
        "            dw1 = np.matmul(dz1, features.T)\n",
        "            db1 = np.average(dz1, axis=1)\n",
        "\n",
        "            self.weight2 = self.weight2 - dw2 * learning_rate\n",
        "            self.weight1 = self.weight1 - dw1 * learning_rate\n",
        "            self.bias2 = self.bias2 - db2 * learning_rate\n",
        "            self.bias1 = self.bias1 - db1 * learning_rate\n",
        "\n",
        "            error = self.cost(labels, layer2)\n",
        "            if iter % (EPOCHS / 10) == 0:\n",
        "                print(\"iter: {:4} error: {:10.4f}\".format(iter, error))\n",
        "model = NeuralNetwork_2(len(images_train), 5, 10)\n",
        "model.train(images_train, labels_train, 0.001, 100)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter:    0 error:     2.3499\n",
            "iter:   10 error:     2.3504\n",
            "iter:   20 error:     2.3503\n",
            "iter:   30 error:     2.3502\n",
            "iter:   40 error:     2.3501\n",
            "iter:   50 error:     2.3500\n",
            "iter:   60 error:     2.3499\n",
            "iter:   70 error:     2.3499\n",
            "iter:   80 error:     2.3498\n",
            "iter:   90 error:     2.3497\n",
            "iter:  100 error:     2.3496\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1fziF8c1j7W",
        "outputId": "d49a72aa-66df-460f-866d-94079a2e3bfc"
      },
      "source": [
        "# 2 Layer NN(ReLU, Softmax) with batch\n",
        "import sys\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "(images_train, labels_train), (images_test, labels_test) = tf.keras.datasets.mnist.load_data()\n",
        "images_train = np.reshape(images_train / 255.0, (len(images_train), len(images_train[0]) * len(images_train[0][0]))).T\n",
        "images_test = np.reshape(images_test / 255.0, (len(images_test), len(images_test[0]) * len(images_test[0][0]))).T\n",
        "\n",
        "def one_hot_encoding(data):\n",
        "    encoded = np.zeros((len(data), max(data) - min(data) + 1))\n",
        "    for idx in range(len(data)):\n",
        "        encoded[idx][data[idx]] = 1\n",
        "    return encoded.T\n",
        "\n",
        "labels_train, labels_test = one_hot_encoding(labels_train), one_hot_encoding(labels_test)\n",
        "\n",
        "class NeuralNetwork_2():\n",
        "    def __init__(self, n0, n1, n2):\n",
        "        self.weight1 = np.random.rand(n1, n0) * 0.01\n",
        "        self.weight2 = np.random.rand(n2, n1) * 0.01\n",
        "        self.bias1 = np.random.rand(n1)\n",
        "        self.bias2 = np.random.rand(n2)\n",
        "    def linear_hypo(self, w, x, b):\n",
        "        return (np.matmul(w, x).T + b).T\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "    def relu(self, z):\n",
        "        return np.maximum(z, 0)\n",
        "    def softmax(self, z):\n",
        "        return np.exp(z) / np.exp(z).sum(axis=0)\n",
        "    def hypothesis(self, func, lh):\n",
        "        if func == 'sigmoid':\n",
        "            return self.sigmoid(lh)\n",
        "        elif func == 'relu':\n",
        "            return self.relu(lh)\n",
        "        elif func == 'softmax':\n",
        "            return self.softmax(lh)\n",
        "        else:\n",
        "            sys.exit('Error in hypothesis: There is no {} function'.format(func))\n",
        "    def cost(self, y, hypo):\n",
        "        # cross entropy\n",
        "        sum = np.multiply(y, np.log(hypo)).sum(axis=0)\n",
        "        return -np.average(sum)\n",
        "    def train(self, features, labels, learning_rate, EPOCHS, batch_size):\n",
        "        for iter in range(EPOCHS + 1):\n",
        "            for batch in range(int(len(features[0]) / batch_size)):\n",
        "                features_batch = features[:, batch * batch_size : (batch + 1) * batch_size]\n",
        "                labels_batch = labels[:, batch * batch_size : (batch + 1) * batch_size]\n",
        "                # layer1: relu, layer2: softmax, cost: cross-entropy\n",
        "                z1 = self.linear_hypo(self.weight1, features_batch, self.bias1)\n",
        "                layer1 = self.hypothesis('relu', z1)\n",
        "                z2 = self.linear_hypo(self.weight2, layer1, self.bias2)\n",
        "                layer2 = self.hypothesis('softmax', z2)\n",
        "                # gradient\n",
        "                dz2 = layer2 - labels_batch\n",
        "                dw2 = np.matmul(dz2, layer1.T)\n",
        "                db2 = np.average(dz2, axis=1)\n",
        "\n",
        "                dz1 = np.multiply(np.matmul(self.weight2.T, dz2), np.where(z1 > 0, 1, 0)) # relu gradient: if x > 0: 1 else: 0 => np.where(z1 > 0, 1, 0)\n",
        "                dw1 = np.matmul(dz1, features_batch.T)\n",
        "                db1 = np.average(dz1, axis=1)\n",
        "\n",
        "                self.weight2 = self.weight2 - dw2 * learning_rate\n",
        "                self.weight1 = self.weight1 - dw1 * learning_rate\n",
        "                self.bias2 = self.bias2 - db2 * learning_rate\n",
        "                self.bias1 = self.bias1 - db1 * learning_rate\n",
        "\n",
        "                error = self.cost(labels_batch, layer2)\n",
        "            #if iter % (EPOCHS / 10) == 0:\n",
        "            #    print(\"iter: {:4} error: {:10.4f}\".format(iter, error))\n",
        "    def test_accuracy(self, features, labels):\n",
        "        hypothesis = self.hypothesis('softmax', self.linear_hypo(self.weight2, self.hypothesis('relu', self.linear_hypo(self.weight1, features, self.bias1)), self.bias2))\n",
        "        prob = np.average((np.argmax(hypothesis, axis=0) == np.argmax(labels, axis=0))) * 100\n",
        "        print(\"Test Accuracy: {:10.4f}\".format(prob))\n",
        "model = NeuralNetwork_2(len(images_train), 5, 10)\n",
        "model.train(images_train, labels_train, 0.01, 100, 100)\n",
        "model.test_accuracy(images_test, labels_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "Test Accuracy:    11.3500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2f_Wu31nfSx"
      },
      "source": [
        "#Find best setting of model evaluated by Test Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6JPTx7NKl6u",
        "outputId": "d500493c-701d-475e-8c8c-173c6ed6da8b"
      },
      "source": [
        "# Test Accuracy comparison with changing learning_rate - 0.0001 is the best\n",
        "model = NeuralNetwork_2(len(images_train), 5, 10)\n",
        "model.train(images_train, labels_train, 0.1, 100, 50)\n",
        "model.test_accuracy(images_test, labels_test)\n",
        "\n",
        "model = NeuralNetwork_2(len(images_train), 5, 10)\n",
        "model.train(images_train, labels_train, 0.01, 100, 50)\n",
        "model.test_accuracy(images_test, labels_test)\n",
        "\n",
        "model = NeuralNetwork_2(len(images_train), 5, 10)\n",
        "model.train(images_train, labels_train, 0.001, 100, 50)\n",
        "model.test_accuracy(images_test, labels_test)\n",
        "\n",
        "model = NeuralNetwork_2(len(images_train), 5, 10)\n",
        "model.train(images_train, labels_train, 0.0001, 100, 50)\n",
        "model.test_accuracy(images_test, labels_test)\n",
        "\n",
        "model = NeuralNetwork_2(len(images_train), 5, 10)\n",
        "model.train(images_train, labels_train, 0.00001, 100, 50)\n",
        "model.test_accuracy(images_test, labels_test)\n",
        "\n",
        "model = NeuralNetwork_2(len(images_train), 5, 10)\n",
        "model.train(images_train, labels_train, 0.000001, 100, 50)\n",
        "model.test_accuracy(images_test, labels_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy:    10.2800\n",
            "Test Accuracy:    86.4100\n",
            "Test Accuracy:    88.7300\n",
            "Test Accuracy:    89.4500\n",
            "Test Accuracy:    86.4200\n",
            "Test Accuracy:    12.1900\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dkAnpq0NRYX",
        "outputId": "5ad63212-ec95-4030-8bd5-69fa05a01915"
      },
      "source": [
        "# Test Accuracy comparison with changing batch_size - 100 is the best\n",
        "model = NeuralNetwork_2(len(images_train), 5, 10)\n",
        "model.train(images_train, labels_train, 0.0001, 100, 10)\n",
        "model.test_accuracy(images_test, labels_test)\n",
        "\n",
        "model = NeuralNetwork_2(len(images_train), 5, 10)\n",
        "model.train(images_train, labels_train, 0.0001, 100, 50)\n",
        "model.test_accuracy(images_test, labels_test)\n",
        "\n",
        "model = NeuralNetwork_2(len(images_train), 5, 10)\n",
        "model.train(images_train, labels_train, 0.0001, 100, 100)\n",
        "model.test_accuracy(images_test, labels_test)\n",
        "\n",
        "model = NeuralNetwork_2(len(images_train), 5, 10)\n",
        "model.train(images_train, labels_train, 0.0001, 100, 1000)\n",
        "model.test_accuracy(images_test, labels_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy:    89.5300\n",
            "Test Accuracy:    89.5200\n",
            "Test Accuracy:    89.7100\n",
            "Test Accuracy:    89.6400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXQ3YnrLOQut",
        "outputId": "50db9c49-fae4-4b5c-fa30-3592bad430a9"
      },
      "source": [
        "# Test Accuracy comparison with changing number of hidden layer nodes - 100 is the best\n",
        "model = NeuralNetwork_2(len(images_train), 20, 10)\n",
        "model.train(images_train, labels_train, 0.0001, 100, 100)\n",
        "model.test_accuracy(images_test, labels_test)\n",
        "\n",
        "model = NeuralNetwork_2(len(images_train), 50, 10)\n",
        "model.train(images_train, labels_train, 0.0001, 100, 100)\n",
        "model.test_accuracy(images_test, labels_test)\n",
        "\n",
        "model = NeuralNetwork_2(len(images_train), 100, 10)\n",
        "model.train(images_train, labels_train, 0.0001, 100, 100)\n",
        "model.test_accuracy(images_test, labels_test)\n",
        "\n",
        "model = NeuralNetwork_2(len(images_train), 1000, 10) # too much time needed to calculate\n",
        "model.train(images_train, labels_train, 0.0001, 100, 100)\n",
        "model.test_accuracy(images_test, labels_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy:    95.2200\n",
            "Test Accuracy:    95.9300\n",
            "Test Accuracy:    96.3200\n",
            "Test Accuracy:    95.4800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iof00sGOYJ6l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21bf8bdc-7917-4bdc-811b-9d5a8fb8349a"
      },
      "source": [
        "# Test Accuracy comparison with changing EPOCHS - as many as possible\n",
        "# 100 nodes of hidden layer's calculation takes too much time so decreased it to 20\n",
        "model = NeuralNetwork_2(len(images_train), 20, 10)\n",
        "model.train(images_train, labels_train, 0.0001, 10, 100)\n",
        "model.test_accuracy(images_test, labels_test)\n",
        "\n",
        "model = NeuralNetwork_2(len(images_train), 20, 10)\n",
        "model.train(images_train, labels_train, 0.0001, 100, 100)\n",
        "model.test_accuracy(images_test, labels_test)\n",
        "\n",
        "model = NeuralNetwork_2(len(images_train), 20, 10)\n",
        "model.train(images_train, labels_train, 0.0001, 1000, 100)\n",
        "model.test_accuracy(images_test, labels_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy:    90.9200\n",
            "Test Accuracy:    94.8000\n",
            "Test Accuracy:    95.8800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMLuJHM0V8WK",
        "outputId": "e0535125-aea2-477a-f134-647d1f070cb6"
      },
      "source": [
        "# 3 Layer NN(ReLU, Softmax) with no batch\n",
        "import sys\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "(images_train, labels_train), (images_test, labels_test) = tf.keras.datasets.mnist.load_data()\n",
        "images_train = np.reshape(images_train / 255.0, (len(images_train), len(images_train[0]) * len(images_train[0][0]))).T\n",
        "images_test = np.reshape(images_test / 255.0, (len(images_test), len(images_test[0]) * len(images_test[0][0]))).T\n",
        "\n",
        "def one_hot_encoding(data):\n",
        "    encoded = np.zeros((len(data), max(data) - min(data) + 1))\n",
        "    for idx in range(len(data)):\n",
        "        encoded[idx][data[idx]] = 1\n",
        "    return encoded.T\n",
        "\n",
        "labels_train, labels_test = one_hot_encoding(labels_train), one_hot_encoding(labels_test)\n",
        "\n",
        "class NeuralNetwork_3():\n",
        "    def __init__(self, n0, n1, n2, n3):\n",
        "        self.weight1 = np.random.rand(n1, n0)\n",
        "        self.weight2 = np.random.rand(n2, n1)\n",
        "        self.weight3 = np.random.rand(n3, n2)\n",
        "        self.bias1 = np.random.rand(n1)\n",
        "        self.bias2 = np.random.rand(n2)\n",
        "        self.bias3 = np.random.rand(n3)\n",
        "    def linear_hypo(self, w, x, b):\n",
        "        return (np.matmul(w, x).T + b).T\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "    def relu(self, z):\n",
        "        return np.maximum(z, 0)\n",
        "    def softmax(self, z):\n",
        "        return np.exp(z) / np.exp(z).sum(axis=0)\n",
        "    def hypothesis(self, func, lh):\n",
        "        if func == 'sigmoid':\n",
        "            return self.sigmoid(lh)\n",
        "        elif func == 'relu':\n",
        "            return self.relu(lh)\n",
        "        elif func == 'softmax':\n",
        "            return self.softmax(lh)\n",
        "        else:\n",
        "            sys.exit('Error in hypothesis: There is no {} function'.format(func))\n",
        "    def cost(self, y, hypo):\n",
        "        # cross entropy\n",
        "        sum = np.multiply(y, np.log(hypo)).sum(axis=0)\n",
        "        return -np.average(sum)\n",
        "    def train(self, features, labels, learning_rate, EPOCHS, batch_size):\n",
        "        for iter in range(EPOCHS + 1):\n",
        "            for batch in range(int(len(features[0]) / batch_size)):\n",
        "                features_batch = features[:, batch * batch_size : (batch + 1) * batch_size]\n",
        "                labels_batch = labels[:, batch * batch_size : (batch + 1) * batch_size]\n",
        "                # layer1: relu, layer2: relu, layer3: softmax, cost: cross-entropy\n",
        "                z1 = self.linear_hypo(self.weight1, features_batch, self.bias1)\n",
        "                layer1 = self.hypothesis('relu', z1)\n",
        "                z2 = self.linear_hypo(self.weight2, layer1, self.bias2)\n",
        "                layer2 = self.hypothesis('relu', z2)\n",
        "                z3 = self.linear_hypo(self.weight3, layer2, self.bias3)\n",
        "                layer3 = self.hypothesis('softmax', z3)\n",
        "                # gradient\n",
        "                dz3 = layer3 - labels_batch\n",
        "                dw3 = np.matmul(dz3, layer2.T)\n",
        "                db3 = np.average(dz3, axis=1)\n",
        "\n",
        "                dz2 = np.multiply(np.matmul(self.weight3.T, dz3), np.where(z2 > 0, 1, 0))\n",
        "                dw2 = np.matmul(dz2, layer1.T)\n",
        "                db2 = np.average(dz2, axis=1)\n",
        "\n",
        "                dz1 = np.multiply(np.matmul(self.weight2.T, dz2), np.where(z1 > 0, 1, 0))\n",
        "                dw1 = np.matmul(dz1, features_batch.T)\n",
        "                db1 = np.average(dz1, axis=1)\n",
        "\n",
        "                self.weight3 = self.weight3 - dw3 * learning_rate\n",
        "                self.weight2 = self.weight2 - dw2 * learning_rate\n",
        "                self.weight1 = self.weight1 - dw1 * learning_rate\n",
        "                self.bias3 = self.bias3 - db3 * learning_rate\n",
        "                self.bias2 = self.bias2 - db2 * learning_rate\n",
        "                self.bias1 = self.bias1 - db1 * learning_rate\n",
        "\n",
        "                error = self.cost(labels_batch, layer3)\n",
        "            if iter % (EPOCHS / 10) == 0:\n",
        "                print(\"iter: {:4} error: {:10.4f}\".format(iter, error))\n",
        "    def test_accuracy(self, features, labels):\n",
        "        layer1 = self.linear_hypo(self.weight1, features, self.bias1)\n",
        "        layer1 = self.hypothesis('relu', layer1)\n",
        "        layer2 = self.linear_hypo(self.weight2, layer1, self.bias2)\n",
        "        layer2 = self.hypothesis('relu', layer2)\n",
        "        layer3 = self.linear_hypo(self.weight3, layer2, self.bias3)\n",
        "        hypothesis = self.hypothesis('softmax', layer3)\n",
        "        prob = np.average((np.argmax(hypothesis, axis=0) == np.argmax(labels, axis=0))) * 100\n",
        "        print(\"Test Accuracy: {:10.4f}\".format(prob))\n",
        "model = NeuralNetwork_3(len(images_train), 5, 5, 10)\n",
        "model.train(images_train, labels_train, 0.0001, 10, 10)\n",
        "model.test_accuracy(images_test, labels_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: RuntimeWarning: overflow encountered in exp\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iter:    0 error:        nan\n",
            "iter:    1 error:        nan\n",
            "iter:    2 error:        nan\n",
            "iter:    3 error:        nan\n",
            "iter:    4 error:        nan\n",
            "iter:    5 error:        nan\n",
            "iter:    6 error:        nan\n",
            "iter:    7 error:        nan\n",
            "iter:    8 error:        nan\n",
            "iter:    9 error:        nan\n",
            "iter:   10 error:        nan\n",
            "Test Accuracy:     9.8000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0SzWmSS8-FI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}