{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralNetwork.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Uv7PokoRIddX",
        "0rDSqxaA7xhw",
        "1DUUkPbH73o-"
      ],
      "authorship_tag": "ABX9TyOMq/qMo/KI6Kr3Nu4+rOE0"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uv7PokoRIddX"
      },
      "source": [
        "#Based on \"Neural Networks and Deep Learning\" Course by Deeplearning.ai"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-W7u_QmtIlK",
        "outputId": "fcb0571b-7a62-4fcd-a9c6-fa46aaddb557"
      },
      "source": [
        "# 2 Layer NN(ReLU, Softmax) with no batch\n",
        "import sys\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "(images_train, labels_train), (images_test, labels_test) = tf.keras.datasets.mnist.load_data()\n",
        "images_train = np.reshape(images_train / 255.0, (len(images_train), len(images_train[0]) * len(images_train[0][0]))).T\n",
        "images_test = np.reshape(images_test / 255.0, (len(images_test), len(images_test[0]) * len(images_test[0][0]))).T\n",
        "\n",
        "def one_hot_encoding(data):\n",
        "    encoded = np.zeros((len(data), max(data) - min(data) + 1))\n",
        "    for idx in range(len(data)):\n",
        "        encoded[idx][data[idx]] = 1\n",
        "    return encoded.T\n",
        "\n",
        "labels_train, labels_test = one_hot_encoding(labels_train), one_hot_encoding(labels_test)\n",
        "\n",
        "class NeuralNetwork_2():\n",
        "    def __init__(self, n0, n1, n2):\n",
        "        self.weight1 = np.random.rand(n1, n0) * 0.01\n",
        "        self.weight2 = np.random.rand(n2, n1) * 0.01\n",
        "        self.bias1 = np.random.rand(n1)\n",
        "        self.bias2 = np.random.rand(n2)\n",
        "    def linear_hypo(self, w, x, b):\n",
        "        return (np.matmul(w, x).T + b).T\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "    def relu(self, z):\n",
        "        return np.maximum(z, 0)\n",
        "    def softmax(self, z):\n",
        "        return np.exp(z) / np.exp(z).sum(axis=0)\n",
        "    def hypothesis(self, func, lh):\n",
        "        if func == 'sigmoid':\n",
        "            return self.sigmoid(lh)\n",
        "        elif func == 'relu':\n",
        "            return self.relu(lh)\n",
        "        elif func == 'softmax':\n",
        "            return self.softmax(lh)\n",
        "        else:\n",
        "            sys.exit('Error in hypothesis: There is no {} function'.format(func))\n",
        "    def cost(self, y, hypo):\n",
        "        # cross entropy\n",
        "        sum = np.multiply(y, np.log(hypo)).sum(axis=0)\n",
        "        return -np.average(sum)\n",
        "    def train(self, features, labels, learning_rate, EPOCHS):\n",
        "        for iter in range(EPOCHS + 1):\n",
        "            # layer1: relu, layer2: softmax, cost: cross-entropy\n",
        "            z1 = self.linear_hypo(self.weight1, features, self.bias1)\n",
        "            layer1 = self.hypothesis('relu', z1)\n",
        "            z2 = self.linear_hypo(self.weight2, layer1, self.bias2)\n",
        "            layer2 = self.hypothesis('softmax', z2)\n",
        "            # gradient\n",
        "            dz2 = layer2 - labels\n",
        "            dw2 = np.matmul(dz2, layer1.T)\n",
        "            db2 = np.average(dz2, axis=1)\n",
        "\n",
        "            dz1 = np.multiply(np.matmul(self.weight2.T, dz2), np.where(z1 > 0, 1, 0)) # relu gradient: if x > 0: 1 else: 0 => np.where(z1 > 0, 1, 0)\n",
        "            dw1 = np.matmul(dz1, features.T)\n",
        "            db1 = np.average(dz1, axis=1)\n",
        "\n",
        "            self.weight2 = self.weight2 - dw2 * learning_rate\n",
        "            self.weight1 = self.weight1 - dw1 * learning_rate\n",
        "            self.bias2 = self.bias2 - db2 * learning_rate\n",
        "            self.bias1 = self.bias1 - db1 * learning_rate\n",
        "\n",
        "            error = self.cost(labels, layer2)\n",
        "            if iter % (EPOCHS / 10) == 0:\n",
        "                print(\"iter: {:4} error: {:10.4f}\".format(iter, error))\n",
        "model = NeuralNetwork_2(len(images_train), 5, 10)\n",
        "model.train(images_train, labels_train, 0.001, 100)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter:    0 error:     2.3499\n",
            "iter:   10 error:     2.3504\n",
            "iter:   20 error:     2.3503\n",
            "iter:   30 error:     2.3502\n",
            "iter:   40 error:     2.3501\n",
            "iter:   50 error:     2.3500\n",
            "iter:   60 error:     2.3499\n",
            "iter:   70 error:     2.3499\n",
            "iter:   80 error:     2.3498\n",
            "iter:   90 error:     2.3497\n",
            "iter:  100 error:     2.3496\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1fziF8c1j7W",
        "outputId": "f5077bd8-f417-4273-a0b8-38e86b2f15ba"
      },
      "source": [
        "# 2 Layer NN(ReLU, Softmax) with batch\n",
        "import sys\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "(images_train, labels_train), (images_test, labels_test) = tf.keras.datasets.mnist.load_data()\n",
        "images_train = np.reshape(images_train / 255.0, (len(images_train), len(images_train[0]) * len(images_train[0][0]))).T\n",
        "images_test = np.reshape(images_test / 255.0, (len(images_test), len(images_test[0]) * len(images_test[0][0]))).T\n",
        "\n",
        "def one_hot_encoding(data):\n",
        "    encoded = np.zeros((len(data), max(data) - min(data) + 1))\n",
        "    for idx in range(len(data)):\n",
        "        encoded[idx][data[idx]] = 1\n",
        "    return encoded.T\n",
        "\n",
        "labels_train, labels_test = one_hot_encoding(labels_train), one_hot_encoding(labels_test)\n",
        "\n",
        "class NeuralNetwork_2():\n",
        "    def __init__(self, n0, n1, n2):\n",
        "        self.weight1 = np.random.rand(n1, n0) * 0.01\n",
        "        self.weight2 = np.random.rand(n2, n1) * 0.01\n",
        "        self.bias1 = np.random.rand(n1)\n",
        "        self.bias2 = np.random.rand(n2)\n",
        "    def linear_hypo(self, w, x, b):\n",
        "        return (np.matmul(w, x).T + b).T\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "    def relu(self, z):\n",
        "        return np.maximum(z, 0)\n",
        "    def softmax(self, z):\n",
        "        return np.exp(z) / np.exp(z).sum(axis=0) \n",
        "    def hypothesis(self, func, lh):\n",
        "        if func == 'sigmoid':\n",
        "            return self.sigmoid(lh)\n",
        "        elif func == 'relu':\n",
        "            return self.relu(lh)\n",
        "        elif func == 'softmax':\n",
        "            return self.softmax(lh)\n",
        "        else:\n",
        "            sys.exit('Error in hypothesis: There is no {} function'.format(func))\n",
        "    def cost(self, y, hypo):\n",
        "        # cross entropy\n",
        "        sum = np.multiply(y, np.log(hypo)).sum(axis=0)\n",
        "        return -np.average(sum)\n",
        "    def train(self, features, labels, learning_rate, EPOCHS, batch_size):\n",
        "        for iter in range(EPOCHS + 1):\n",
        "            for batch in range(int(len(features[0]) / batch_size)):\n",
        "                features_batch = features[:, batch * batch_size : (batch + 1) * batch_size]\n",
        "                labels_batch = labels[:, batch * batch_size : (batch + 1) * batch_size]\n",
        "                # layer1: relu, layer2: softmax, cost: cross-entropy\n",
        "                z1 = self.linear_hypo(self.weight1, features_batch, self.bias1)\n",
        "                layer1 = self.hypothesis('relu', z1)\n",
        "                z2 = self.linear_hypo(self.weight2, layer1, self.bias2)\n",
        "                layer2 = self.hypothesis('softmax', z2)\n",
        "                # gradient\n",
        "                dz2 = layer2 - labels_batch #this is same result when you use sigmoid and sigmoid's cost function(y*log(hypo) + (1-y)*log(1-hypo))\n",
        "                dw2 = np.matmul(dz2, layer1.T)\n",
        "                db2 = np.average(dz2, axis=1)\n",
        "\n",
        "                dz1 = np.multiply(np.matmul(self.weight2.T, dz2), np.where(z1 > 0, 1, 0)) # relu gradient: if x > 0: 1 else: 0 => np.where(z1 > 0, 1, 0)\n",
        "                dw1 = np.matmul(dz1, features_batch.T)\n",
        "                db1 = np.average(dz1, axis=1)\n",
        "\n",
        "                self.weight2 = self.weight2 - dw2 * learning_rate\n",
        "                self.weight1 = self.weight1 - dw1 * learning_rate\n",
        "                self.bias2 = self.bias2 - db2 * learning_rate\n",
        "                self.bias1 = self.bias1 - db1 * learning_rate\n",
        "\n",
        "                error = self.cost(labels_batch, layer2)\n",
        "            #if iter % (EPOCHS / 10) == 0:\n",
        "            #    print(\"iter: {:4} error: {:10.4f}\".format(iter, error))\n",
        "    def test_accuracy(self, features, labels):\n",
        "        hypothesis = self.hypothesis('softmax', self.linear_hypo(self.weight2, self.hypothesis('relu', self.linear_hypo(self.weight1, features, self.bias1)), self.bias2))\n",
        "        prob = np.average((np.argmax(hypothesis, axis=0) == np.argmax(labels, axis=0))) * 100\n",
        "        print(\"Test Accuracy: {:10.4f}\".format(prob))\n",
        "model = NeuralNetwork_2(len(images_train), 100, 10)\n",
        "model.train(images_train, labels_train, 0.0001, 100, 100)\n",
        "model.test_accuracy(images_test, labels_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "Test Accuracy:    96.5800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2f_Wu31nfSx"
      },
      "source": [
        "###Find best setting of hyper-parameters evaluated by Test Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6JPTx7NKl6u",
        "outputId": "d500493c-701d-475e-8c8c-173c6ed6da8b"
      },
      "source": [
        "# Test Accuracy comparison with changing learning_rate - 0.0001 is the best\n",
        "model = NeuralNetwork_2(len(images_train), 5, 10)\n",
        "model.train(images_train, labels_train, 0.1, 100, 50)\n",
        "model.test_accuracy(images_test, labels_test)\n",
        "\n",
        "model = NeuralNetwork_2(len(images_train), 5, 10)\n",
        "model.train(images_train, labels_train, 0.01, 100, 50)\n",
        "model.test_accuracy(images_test, labels_test)\n",
        "\n",
        "model = NeuralNetwork_2(len(images_train), 5, 10)\n",
        "model.train(images_train, labels_train, 0.001, 100, 50)\n",
        "model.test_accuracy(images_test, labels_test)\n",
        "\n",
        "model = NeuralNetwork_2(len(images_train), 5, 10)\n",
        "model.train(images_train, labels_train, 0.0001, 100, 50)\n",
        "model.test_accuracy(images_test, labels_test)\n",
        "\n",
        "model = NeuralNetwork_2(len(images_train), 5, 10)\n",
        "model.train(images_train, labels_train, 0.00001, 100, 50)\n",
        "model.test_accuracy(images_test, labels_test)\n",
        "\n",
        "model = NeuralNetwork_2(len(images_train), 5, 10)\n",
        "model.train(images_train, labels_train, 0.000001, 100, 50)\n",
        "model.test_accuracy(images_test, labels_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy:    10.2800\n",
            "Test Accuracy:    86.4100\n",
            "Test Accuracy:    88.7300\n",
            "Test Accuracy:    89.4500\n",
            "Test Accuracy:    86.4200\n",
            "Test Accuracy:    12.1900\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dkAnpq0NRYX",
        "outputId": "5ad63212-ec95-4030-8bd5-69fa05a01915"
      },
      "source": [
        "# Test Accuracy comparison with changing batch_size - 100 is the best\n",
        "model = NeuralNetwork_2(len(images_train), 5, 10)\n",
        "model.train(images_train, labels_train, 0.0001, 100, 10)\n",
        "model.test_accuracy(images_test, labels_test)\n",
        "\n",
        "model = NeuralNetwork_2(len(images_train), 5, 10)\n",
        "model.train(images_train, labels_train, 0.0001, 100, 50)\n",
        "model.test_accuracy(images_test, labels_test)\n",
        "\n",
        "model = NeuralNetwork_2(len(images_train), 5, 10)\n",
        "model.train(images_train, labels_train, 0.0001, 100, 100)\n",
        "model.test_accuracy(images_test, labels_test)\n",
        "\n",
        "model = NeuralNetwork_2(len(images_train), 5, 10)\n",
        "model.train(images_train, labels_train, 0.0001, 100, 1000)\n",
        "model.test_accuracy(images_test, labels_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy:    89.5300\n",
            "Test Accuracy:    89.5200\n",
            "Test Accuracy:    89.7100\n",
            "Test Accuracy:    89.6400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXQ3YnrLOQut",
        "outputId": "50db9c49-fae4-4b5c-fa30-3592bad430a9"
      },
      "source": [
        "# Test Accuracy comparison with changing number of hidden layer nodes - 100 is the best\n",
        "model = NeuralNetwork_2(len(images_train), 20, 10)\n",
        "model.train(images_train, labels_train, 0.0001, 100, 100)\n",
        "model.test_accuracy(images_test, labels_test)\n",
        "\n",
        "model = NeuralNetwork_2(len(images_train), 50, 10)\n",
        "model.train(images_train, labels_train, 0.0001, 100, 100)\n",
        "model.test_accuracy(images_test, labels_test)\n",
        "\n",
        "model = NeuralNetwork_2(len(images_train), 100, 10)\n",
        "model.train(images_train, labels_train, 0.0001, 100, 100)\n",
        "model.test_accuracy(images_test, labels_test)\n",
        "\n",
        "model = NeuralNetwork_2(len(images_train), 1000, 10) # too much time needed to calculate\n",
        "model.train(images_train, labels_train, 0.0001, 100, 100)\n",
        "model.test_accuracy(images_test, labels_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy:    95.2200\n",
            "Test Accuracy:    95.9300\n",
            "Test Accuracy:    96.3200\n",
            "Test Accuracy:    95.4800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iof00sGOYJ6l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21bf8bdc-7917-4bdc-811b-9d5a8fb8349a"
      },
      "source": [
        "# Test Accuracy comparison with changing EPOCHS - as many as possible\n",
        "# 100 nodes of hidden layer's calculation takes too much time so decreased it to 20\n",
        "model = NeuralNetwork_2(len(images_train), 20, 10)\n",
        "model.train(images_train, labels_train, 0.0001, 10, 100)\n",
        "model.test_accuracy(images_test, labels_test)\n",
        "\n",
        "model = NeuralNetwork_2(len(images_train), 20, 10)\n",
        "model.train(images_train, labels_train, 0.0001, 100, 100)\n",
        "model.test_accuracy(images_test, labels_test)\n",
        "\n",
        "model = NeuralNetwork_2(len(images_train), 20, 10)\n",
        "model.train(images_train, labels_train, 0.0001, 1000, 100)\n",
        "model.test_accuracy(images_test, labels_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy:    90.9200\n",
            "Test Accuracy:    94.8000\n",
            "Test Accuracy:    95.8800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "396tBgWwYq-x"
      },
      "source": [
        "###3 Layer NN with batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMLuJHM0V8WK",
        "outputId": "0be55a31-4a43-47a5-e4e4-c3dc6d80ccbd"
      },
      "source": [
        "# 3 Layer NN(ReLU, Softmax) with batch\n",
        "import sys\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "(images_train, labels_train), (images_test, labels_test) = tf.keras.datasets.mnist.load_data()\n",
        "images_train = np.reshape(images_train / 255.0, (len(images_train), len(images_train[0]) * len(images_train[0][0]))).T\n",
        "images_test = np.reshape(images_test / 255.0, (len(images_test), len(images_test[0]) * len(images_test[0][0]))).T\n",
        "\n",
        "def one_hot_encoding(data):\n",
        "    encoded = np.zeros((len(data), max(data) - min(data) + 1))\n",
        "    for idx in range(len(data)):\n",
        "        encoded[idx][data[idx]] = 1\n",
        "    return encoded.T\n",
        "\n",
        "labels_train, labels_test = one_hot_encoding(labels_train), one_hot_encoding(labels_test)\n",
        "\n",
        "class NeuralNetwork_3():\n",
        "    def __init__(self, n0, n1, n2, n3):\n",
        "        self.weight1 = np.random.rand(n1, n0) * 0.01\n",
        "        self.weight2 = np.random.rand(n2, n1) * 0.01\n",
        "        self.weight3 = np.random.rand(n3, n2) * 0.01\n",
        "        self.bias1 = np.random.rand(n1)\n",
        "        self.bias2 = np.random.rand(n2)\n",
        "        self.bias3 = np.random.rand(n3)\n",
        "    def linear_hypo(self, w, x, b):\n",
        "        return (np.matmul(w, x).T + b).T\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "    def relu(self, z):\n",
        "        return np.maximum(z, 0)\n",
        "    def softmax(self, z):\n",
        "        return np.exp(z) / np.exp(z).sum(axis=0)\n",
        "    def hypothesis(self, func, lh):\n",
        "        if func == 'sigmoid':\n",
        "            return self.sigmoid(lh)\n",
        "        elif func == 'relu':\n",
        "            return self.relu(lh)\n",
        "        elif func == 'softmax':\n",
        "            return self.softmax(lh)\n",
        "        else:\n",
        "            sys.exit('Error in hypothesis: There is no {} function'.format(func))\n",
        "    def cost(self, y, hypo):\n",
        "        # cross entropy\n",
        "        sum = np.multiply(y, np.log(hypo)).sum(axis=0)\n",
        "        return -np.average(sum)\n",
        "    def train(self, features, labels, learning_rate, EPOCHS, batch_size):\n",
        "        for iter in range(EPOCHS + 1):\n",
        "            for batch in range(int(len(features[0]) / batch_size)):\n",
        "                features_batch = features[:, batch * batch_size : (batch + 1) * batch_size]\n",
        "                labels_batch = labels[:, batch * batch_size : (batch + 1) * batch_size]\n",
        "                # layer1: relu, layer2: relu, layer3: softmax, cost: cross-entropy\n",
        "                z1 = self.linear_hypo(self.weight1, features_batch, self.bias1)\n",
        "                layer1 = self.hypothesis('relu', z1)\n",
        "                z2 = self.linear_hypo(self.weight2, layer1, self.bias2)\n",
        "                layer2 = self.hypothesis('relu', z2)\n",
        "                z3 = self.linear_hypo(self.weight3, layer2, self.bias3)\n",
        "                layer3 = self.hypothesis('softmax', z3)\n",
        "                # gradient\n",
        "                dz3 = layer3 - labels_batch\n",
        "                dw3 = np.matmul(dz3, layer2.T)\n",
        "                db3 = np.average(dz3, axis=1)\n",
        "\n",
        "                dz2 = np.multiply(np.matmul(self.weight3.T, dz3), np.where(z2 > 0, 1, 0))\n",
        "                dw2 = np.matmul(dz2, layer1.T)\n",
        "                db2 = np.average(dz2, axis=1)\n",
        "\n",
        "                dz1 = np.multiply(np.matmul(self.weight2.T, dz2), np.where(z1 > 0, 1, 0))\n",
        "                dw1 = np.matmul(dz1, features_batch.T)\n",
        "                db1 = np.average(dz1, axis=1)\n",
        "\n",
        "                self.weight3 = self.weight3 - dw3 * learning_rate\n",
        "                self.weight2 = self.weight2 - dw2 * learning_rate\n",
        "                self.weight1 = self.weight1 - dw1 * learning_rate\n",
        "                self.bias3 = self.bias3 - db3 * learning_rate\n",
        "                self.bias2 = self.bias2 - db2 * learning_rate\n",
        "                self.bias1 = self.bias1 - db1 * learning_rate\n",
        "\n",
        "                error = self.cost(labels_batch, layer3)\n",
        "            if iter % (EPOCHS / 10) == 0:\n",
        "                print(\"iter: {:4} error: {:10.4f}\".format(iter, error))\n",
        "    def test_accuracy(self, features, labels):\n",
        "        layer1 = self.linear_hypo(self.weight1, features, self.bias1)\n",
        "        layer1 = self.hypothesis('relu', layer1)\n",
        "        layer2 = self.linear_hypo(self.weight2, layer1, self.bias2)\n",
        "        layer2 = self.hypothesis('relu', layer2)\n",
        "        layer3 = self.linear_hypo(self.weight3, layer2, self.bias3)\n",
        "        hypothesis = self.hypothesis('softmax', layer3)\n",
        "        prob = np.average((np.argmax(hypothesis, axis=0) == np.argmax(labels, axis=0))) * 100\n",
        "        print(\"Test Accuracy: {:10.4f}\".format(prob))\n",
        "model = NeuralNetwork_3(len(images_train), 100, 100, len(labels_train))\n",
        "model.train(images_train, labels_train, 0.0001, 100, 100)\n",
        "model.test_accuracy(images_test, labels_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "iter:    0 error:     2.2831\n",
            "iter:   10 error:     1.0475\n",
            "iter:   20 error:     0.4677\n",
            "iter:   30 error:     0.2965\n",
            "iter:   40 error:     0.2318\n",
            "iter:   50 error:     0.2020\n",
            "iter:   60 error:     0.1896\n",
            "iter:   70 error:     0.1801\n",
            "iter:   80 error:     0.1705\n",
            "iter:   90 error:     0.1593\n",
            "iter:  100 error:     0.1515\n",
            "Test Accuracy:    96.3000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OBqi0TcTX1_"
      },
      "source": [
        "#Multi Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DUUkPbH73o-"
      },
      "source": [
        "###Try1 => Failed(Error in Backprop)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puhp2NkT9Pxb"
      },
      "source": [
        "# x => z = wx + b => a = activation_func(z)\n",
        "# a_l-1 => z_l = w_l * a_l-1 + b_l => a_l = activation_func_l(z_l)\n",
        "# da <=> dJ / da\n",
        "# J <=> cost function\n",
        "# reference of gradient of cross entropy : https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_minimization => Cross-entropy loss function and logistic regression\n",
        "# => d/db * L(b) = X(Y^ - Y) <==> dw = x * (hypo - label) when J is a cross-entropy function.\n",
        "import numpy as np\n",
        "\n",
        "class Func():\n",
        "    def sigmoid(z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "    def relu(z):\n",
        "        return np.maximum(z, 0)\n",
        "    def softmax(z): #same column: one example => sum(axis=0): sum of all values of one example and divide each classes of one example by this sum\n",
        "        return np.exp(z) / np.exp(z).sum(axis=0) #sum(axis=0) => add same column numbers and so #rows become 1\n",
        "    def softmax_normalized(z):\n",
        "        #since softmax has exp and exp can make numbers too large very easily and it causes overflow in caculation\n",
        "        z = (z - z.min()) / (z.max() - z.min())\n",
        "        #print(\"z:\", z, \"softmax:\", np.exp(z) / np.exp(z).sum(axis=0))\n",
        "        return np.exp(z) / np.exp(z).sum(axis=0)\n",
        "\n",
        "class Layer():\n",
        "    def __init__(self, input_size, output_size, act_func):\n",
        "        self.weight = np.random.rand(output_size, input_size) * 0.01\n",
        "        self.bias = np.random.rand(output_size, 1) * 0.01\n",
        "        self.act_func = act_func\n",
        "        self.z = None # result of linear\n",
        "        self.a = None # result of act_func(linear)\n",
        "        self.dz = None # equals dJ/dz\n",
        "    def linear(self, input):\n",
        "        self.z = np.matmul(self.weight, input) + self.bias #size: (n1, n0) * (n0, m) + (broadcasting)(n1, 1) => (n1, m)\n",
        "        return self.z\n",
        "    def activate_func(self, input):\n",
        "        if self.act_func == 'sigmoid':\n",
        "            self.a = Func.sigmoid(input)\n",
        "        elif self.act_func == 'relu':\n",
        "            self.a = Func.relu(input)\n",
        "        elif self.act_func == 'softmax':\n",
        "            self.a = Func.softmax_normalized(input)\n",
        "        return self.a\n",
        "    def output(self, input):\n",
        "        return self.forward(input)\n",
        "    def forward(self, input):\n",
        "        return self.activate_func(self.linear(input))\n",
        "    def backward(self, da_next, a_before, lr): #next: next layer(outputlayer if this is hidden in 3layerNN), before: before layer(inputlayer if this is hidden in 3layerNN)\n",
        "        # dw_l = da_l * (depending on layer_l's activation func) * a_l-1\n",
        "        # da_l * (dependin on ) part equals dz_l\n",
        "        # if act_func is softmax, dz_l part equals (a_l - y) and so dw_l = matmul((a_l - y), a_l-1.T)\n",
        "        # if act_func is relu, (depending on ) part means a matrix composed of 0 or 1 (if z_l is non-zero, 1 and the others are 0)\n",
        "        # if act_func is relu, da_l equals matmul(dw_l+1.T, dz_l+1)\n",
        "        if self.act_func == 'relu':\n",
        "            dz = np.multiply(da_next, np.where(self.z > 0, 1, 0))\n",
        "        else: #sigmoid or softmax\n",
        "            dz = da_next\n",
        "        self.dz = dz\n",
        "        dw = np.matmul(dz, a_before.T)\n",
        "        db = np.reshape(np.average(dz, axis=1), (len(dz), 1))\n",
        "\n",
        "        self.weight = self.weight - lr * dw\n",
        "        self.db = self.bias - lr * db\n",
        "\n",
        "class NeuralNetwork():\n",
        "    def __init__(self, inputUnits, outputUnits, hiddenUnits, hiddenLayers, activation_func):\n",
        "        self.Layers = [Layer(inputUnits, hiddenUnits, activation_func)]\n",
        "        for i in range(hiddenLayers):\n",
        "            self.Layers.append(Layer(hiddenUnits, hiddenUnits, activation_func))\n",
        "        self.Layers.append(Layer(hiddenUnits, outputUnits, 'softmax'))\n",
        "\n",
        "    def cost(self, hypo, label):\n",
        "        #cross entropy\n",
        "        total = np.multiply(label, np.log(hypo)).sum(axis=0)\n",
        "        return -np.average(total)\n",
        "\n",
        "    def train(self, input_, output_, batch_size, lr, EPOCHS):\n",
        "        for iter in range(EPOCHS + 1):\n",
        "            for i in range(int(len(input_[0]) / batch_size)):\n",
        "                input = input_[:, i * batch_size : (i + 1) * batch_size]\n",
        "                output = output_[:, i * batch_size : (i + 1) * batch_size]\n",
        "\n",
        "                #forward propagation\n",
        "                self.Layers[0].forward(input)\n",
        "                for idx in range(1, len(self.Layers)):\n",
        "                    self.Layers[idx].forward(self.Layers[idx - 1].a)\n",
        "                    if  np.isnan(self.Layers[idx].a).any() or np.isinf(self.Layers[idx].a).any():\n",
        "                        print(\"Forward\")\n",
        "                        print(\"Layer idx:\", idx, \"batch:\", i, \"iter:\", iter, \"weight0:\", self.Layers[0].weight, \"weight1:\", self.Layers[1].weight, \"weight2:\", self.Layers[2].weight)\n",
        "                        print(\"a[0]:\", self.Layers[0].a, \"a[1]:\", self.Layers[1].a, \"a[2]:\", self.Layers[2].a)\n",
        "                        print(\"dz[0]:\", self.Layers[0].dz, \"dz[1]:\", self.Layers[1].dz, \"dz[2]:\", self.Layers[2].dz)\n",
        "                        raise ValueError()\n",
        "                        return None\n",
        "\n",
        "                #Back propagation\n",
        "                idx = len(self.Layers) - 1\n",
        "                self.Layers[-1].backward(self.Layers[-1].a - output, self.Layers[-2].a, lr)\n",
        "                if np.isnan(self.Layers[-1].weight).any() or np.isinf(self.Layers[-1].weight).any():\n",
        "                    print(\"Backprop\")\n",
        "                    print(\"Layer idx:\", idx, \"batch:\", i, \"iter:\", iter, \"weight0:\", self.Layers[0].weight, \"weight1:\", self.Layers[1].weight, \"weight2:\", self.Layers[2].weight)\n",
        "                    print(\"a[0]:\", self.Layers[0].a, \"a[1]:\", self.Layers[1].a, \"a[2]:\", self.Layers[2].a)\n",
        "                    print(\"dz[0]:\", self.Layers[0].dz, \"dz[1]:\", self.Layers[1].dz, \"dz[2]:\", self.Layers[2].dz)\n",
        "                    raise ValueError()\n",
        "                idx = len(self.Layers) - 2\n",
        "                while idx > 0:\n",
        "                    temp = np.matmul(self.Layers[idx + 1].weight.T, self.Layers[idx + 1].dz)\n",
        "                    self.Layers[idx].backward(temp, self.Layers[idx - 1].a, lr)\n",
        "                    if np.isnan(self.Layers[idx].weight).any() or np.isinf(self.Layers[idx].weight).any():\n",
        "                        print(\"Backprop\")\n",
        "                        print(\"Layer idx:\", idx, \"batch:\", i, \"iter:\", iter, \"weight0:\", self.Layers[0].weight, \"weight1:\", self.Layers[1].weight, \"weight2:\", self.Layers[2].weight)\n",
        "                        print(\"a[0]:\", self.Layers[0].a, \"a[1]:\", self.Layers[1].a, \"a[2]:\", self.Layers[2].a)\n",
        "                        print(\"dz[0]:\", self.Layers[0].dz, \"dz[1]:\", self.Layers[1].dz, \"dz[2]:\", self.Layers[2].dz)\n",
        "                        raise ValueError()\n",
        "                    idx = idx - 1\n",
        "                    \n",
        "                temp = np.matmul(self.Layers[1].weight.T, self.Layers[1].dz)\n",
        "                self.Layers[0].backward(temp, input, lr)\n",
        "                if np.isnan(self.Layers[0].weight).any() or np.isinf(self.Layers[0].weight).any():\n",
        "                    print(\"Backprop\")\n",
        "                    print(\"Layer idx:\", idx, \"batch:\", i, \"iter:\", iter, \"weight0:\", self.Layers[0].weight, \"weight1:\", self.Layers[1].weight, \"weight2:\", self.Layers[2].weight)\n",
        "                    print(\"a[0]:\", self.Layers[0].a, \"a[1]:\", self.Layers[1].a, \"a[2]:\", self.Layers[2].a)\n",
        "                    print(\"dz[0]:\", self.Layers[0].dz, \"dz[1]:\", self.Layers[1].dz, \"dz[2]:\", self.Layers[2].dz)\n",
        "                    raise ValueError()\n",
        "                \n",
        "                error = self.cost(self.Layers[-1].a, output)\n",
        "\n",
        "            #print(\"iter:\", iter, \"weight0:\", self.Layers[0].weight, \"weight1:\", self.Layers[1].weight, \"weight2:\", self.Layers[2].weight)\n",
        "            #print(\"iter:\", iter, \"a[0]:\", self.Layers[0].a, \"a[1]:\", self.Layers[1].a, \"a[2]:\", self.Layers[2].a)\n",
        "            #print(\"iter:\", iter, \"dz[0]:\", self.Layers[0].dz, \"dz[1]:\", self.Layers[1].dz, \"dz[2]:\", self.Layers[2].dz)\n",
        "\n",
        "            if iter % (EPOCHS / 10) == 0:\n",
        "                print(\"iter: {:4} error: {:10.4f}\".format(iter, error))\n",
        "\n",
        "    def test_accuracy(self, input, output):\n",
        "        for layer in self.Layers:\n",
        "            if layer == self.Layers[0]:\n",
        "                hypo = layer.forward(input)\n",
        "            else:\n",
        "                hypo = layer.forward(hypo)\n",
        "        prob = np.average((np.argmax(hypo, axis=0) == np.argmax(output, axis=0))) * 100\n",
        "        print(\"Test Accuracy: {:10.4f}\".format(prob))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RKE-bifiiUJ"
      },
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "(images_train, labels_train), (images_test, labels_test) = tf.keras.datasets.mnist.load_data()\n",
        "images_train = np.reshape(images_train / 255.0 / 10, (len(images_train), len(images_train[0]) * len(images_train[0][0]))).T\n",
        "images_test = np.reshape(images_test / 255.0 / 10, (len(images_test), len(images_test[0]) * len(images_test[0][0]))).T\n",
        "\n",
        "def one_hot_encoding(data):\n",
        "    encoded = np.zeros((len(data), max(data) - min(data) + 1))\n",
        "    for idx in range(len(data)):\n",
        "        encoded[idx][data[idx]] = 1\n",
        "    return encoded.T\n",
        "\n",
        "labels_train, labels_test = one_hot_encoding(labels_train), one_hot_encoding(labels_test)\n",
        "model = NeuralNetwork(len(images_train), len(labels_train), hiddenUnits=100, hiddenLayers=1, activation_func='relu')\n",
        "model.train(images_train, labels_train, batch_size=100, lr=0.0001, EPOCHS=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c83HUmaDjShc",
        "outputId": "287ce2b7-048e-4a24-85af-fd1f062a6c06"
      },
      "source": [
        "model.test_accuracy(images_test, labels_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy:     9.9500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irXTzyg776_9"
      },
      "source": [
        "###Try2 => Success"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZedO2e85AR-k"
      },
      "source": [
        "# Multi-Layer NN(ReLU, Softmax) with batch\n",
        "import numpy as np\n",
        "\n",
        "class Func():\n",
        "    def sigmoid(z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "    def relu(z):\n",
        "        return np.maximum(z, 0)\n",
        "    def softmax(z):\n",
        "        return np.exp(z) / np.exp(z).sum(axis=0)\n",
        "\n",
        "class Layer():\n",
        "    def __init__(self, is_output, units_before, units_after):\n",
        "        self.weight = np.random.rand(units_after, units_before) * 0.01\n",
        "        self.bias = np.random.rand(units_after, 1) * 0.01\n",
        "        if is_output:\n",
        "            self.act_func = 'softmax'\n",
        "        else:\n",
        "            self.act_func = 'relu'\n",
        "        self.result = None\n",
        "        self.linear = None\n",
        "    \n",
        "    def linear_func(self, input):\n",
        "        self.linear = np.matmul(self.weight, input) + self.bias\n",
        "        return self.linear\n",
        "    def activation_func(self, input):\n",
        "        if self.act_func == 'softmax':\n",
        "            self.result = Func.softmax(input)\n",
        "        elif self.act_func == 'relu':\n",
        "            self.result = Func.relu(input)\n",
        "        else:\n",
        "            raise ValueError(\"Activation Function named \" + self.act_func + \" is not defined in Func class\")\n",
        "        return self.result\n",
        "    def forward(self, input):\n",
        "        return self.activation_func(self.linear_func(input))\n",
        "\n",
        "class NeuralNetwork_MultiLayers():\n",
        "    # input and hidden layers: relu, output layer: softmax, cost: cross-entropy\n",
        "    def __init__(self, input_units, hidden_units, output_units, hidden_layers):\n",
        "        self.Layers = [Layer(False, input_units, hidden_units)]\n",
        "        for i in range(hidden_layers):\n",
        "            self.Layers.append(Layer(False, hidden_units, hidden_units))\n",
        "        self.Layers.append(Layer(True, hidden_units, output_units))\n",
        "    def cost(self, y, hypo):\n",
        "        # cross entropy\n",
        "        total = np.multiply(y, np.log(hypo)).sum(axis=0)\n",
        "        return -np.average(total)\n",
        "    def train(self, features, labels, lr, EPOCHS, batch_size, print_error):\n",
        "        for iter in range(EPOCHS + 1):\n",
        "            for batch in range(int(len(features[0]) / batch_size)):\n",
        "                features_batch = features[:, batch * batch_size : (batch + 1) * batch_size]\n",
        "                labels_batch = labels[:, batch * batch_size : (batch + 1) * batch_size]\n",
        "                layer_num = len(self.Layers)\n",
        "                # forward\n",
        "                for i in range(layer_num):\n",
        "                    if i == 0:\n",
        "                        self.Layers[i].forward(features_batch)\n",
        "                    else:\n",
        "                        self.Layers[i].forward(self.Layers[i - 1].result)\n",
        "\n",
        "                # gradient(BackProp)\n",
        "                for idx in range(layer_num):\n",
        "                    idx = layer_num - idx - 1\n",
        "                    if idx == layer_num - 1:\n",
        "                        dz = self.Layers[-1].result - labels_batch\n",
        "                    else:\n",
        "                        dz = np.multiply(np.matmul(self.Layers[idx + 1].weight.T, dz), np.where(self.Layers[idx].linear > 0, 1, 0))\n",
        "                    if idx - 1 < 0:\n",
        "                        dw = np.matmul(dz, features_batch.T)\n",
        "                    else:\n",
        "                        dw = np.matmul(dz, self.Layers[idx - 1].result.T)\n",
        "                    db = np.reshape(np.average(dz, axis=1), (len(dz), 1))\n",
        "                    self.Layers[idx].weight -= dw * lr\n",
        "                    self.Layers[idx].bias -= db * lr\n",
        "\n",
        "                error = self.cost(labels_batch, self.Layers[-1].result)\n",
        "                if np.isnan(error).any() or np.isinf(error).any():\n",
        "                    raise ValueError(\"Training Error: Cost value is NAN or INF\")\n",
        "            if print_error:\n",
        "                if iter % (EPOCHS / 10) == 0:\n",
        "                    print(\"iter: {:4} error: {:10.4f}\".format(iter, error))\n",
        "    def test_accuracy(self, features, labels):\n",
        "        for i in range(len(self.Layers)):\n",
        "            if i == 0:\n",
        "                self.Layers[i].forward(features)\n",
        "            else:\n",
        "                self.Layers[i].forward(self.Layers[i - 1].result)\n",
        "        prob = np.average((np.argmax(self.Layers[-1].result, axis=0) == np.argmax(labels, axis=0))) * 100\n",
        "        print(\"Test Accuracy: {:10.4f}\".format(prob))\n",
        "        return prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMpwiEwr25TE"
      },
      "source": [
        "Load MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wMx1V-Z1bIJ"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf # imported for load MNIST data\n",
        "\n",
        "(images_train, labels_train), (images_test, labels_test) = tf.keras.datasets.mnist.load_data()\n",
        "images_train = np.reshape(images_train / 255.0, (len(images_train), len(images_train[0]) * len(images_train[0][0]))).T\n",
        "images_test = np.reshape(images_test / 255.0, (len(images_test), len(images_test[0]) * len(images_test[0][0]))).T\n",
        "\n",
        "def one_hot_encoding(data):\n",
        "    encoded = np.zeros((len(data), max(data) - min(data) + 1))\n",
        "    for idx in range(len(data)):\n",
        "        encoded[idx][data[idx]] = 1\n",
        "    return encoded.T\n",
        "\n",
        "labels_train, labels_test = one_hot_encoding(labels_train), one_hot_encoding(labels_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xvEur543BKN"
      },
      "source": [
        "Init a NN model, train, and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YeaMSA13Ats",
        "outputId": "dc5b2022-b266-4c2d-b9a1-4b457b996c2d"
      },
      "source": [
        "model = NeuralNetwork_MultiLayers(len(images_train), hidden_units=100, output_units=len(labels_train), hidden_layers=2)\n",
        "model.train(images_train, labels_train, 0.0001, EPOCHS=100, batch_size=100, print_error=True)\n",
        "model.test_accuracy(images_test, labels_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter:    0 error:     2.2817\n",
            "iter:   10 error:     1.9758\n",
            "iter:   20 error:     1.9078\n",
            "iter:   30 error:     1.8648\n",
            "iter:   40 error:     1.3606\n",
            "iter:   50 error:     1.0760\n",
            "iter:   60 error:     0.7264\n",
            "iter:   70 error:     0.2138\n",
            "iter:   80 error:     0.1784\n",
            "iter:   90 error:     0.1526\n",
            "iter:  100 error:     0.1362\n",
            "Test Accuracy:    94.1900\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "94.19"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "we36SavO1Lbt"
      },
      "source": [
        "Find Optimal hyper-parameters(learning_rate, batch_size) if the number of hidden layers and hidden units are changed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulRkXsWO1SHz",
        "outputId": "d9fd57d6-79bb-4541-e5ae-53fbe4c1d21e"
      },
      "source": [
        "print(\"Finding optimal learning_rate\")\n",
        "learning_rates = [0.1 / (10 ** i) for i in range(5)]\n",
        "results = []\n",
        "for lr in learning_rates:\n",
        "    model = NeuralNetwork_MultiLayers(len(images_train), hidden_units=100, output_units=len(labels_train), hidden_layers=2) # reset parameters\n",
        "    try:\n",
        "        model.train(images_train, labels_train, lr, EPOCHS=100, batch_size=100, print_error=False)\n",
        "        result = model.test_accuracy(images_test, labels_test)\n",
        "        print(\"lr:\", lr, \"accuracy:\", result)\n",
        "    except:\n",
        "        result = 0\n",
        "        print(\"lr:\", lr, \"Error in training\")\n",
        "    results.append(result)\n",
        "\n",
        "best_lr = learning_rates[np.argmax(results)]\n",
        "print(\"best_lr\", best_lr, \"test result:\", np.max(results))\n",
        "\n",
        "print(\"\\nFinding optimal batch_size\")\n",
        "batch_sizes = [1 * (10 ** i) for i in range(4)]\n",
        "results = []\n",
        "for bs in batch_sizes:\n",
        "    model = NeuralNetwork_MultiLayers(len(images_train), hidden_units=100, output_units=len(labels_train), hidden_layers=2) # reset parameters\n",
        "    try:\n",
        "        model.train(images_train, labels_train, best_lr, EPOCHS=100, batch_size=bs, print_error=False)\n",
        "        result = model.test_accuracy(images_test, labels_test)\n",
        "        print(\"bs:\", bs, \"accuracy:\", result)\n",
        "    except:\n",
        "        result = 0\n",
        "        print(\"bs:\", bs, \"Error in training\")\n",
        "    results.append(result)\n",
        "best_bs = batch_sizes[np.argmax(results)]\n",
        "print(\"best_bs\", best_bs, \"test result:\", np.max(results))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finding optimal learning_rate\n",
            "lr: 0.1 Error in training\n",
            "lr: 0.01 Error in training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: overflow encountered in exp\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in true_divide\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "lr: 0.001 Error in training\n",
            "Test Accuracy:    93.9700\n",
            "lr: 0.0001 accuracy: 93.97\n",
            "Test Accuracy:    20.9400\n",
            "lr: 1e-05 accuracy: 20.94\n",
            "best_lr 0.0001 test result: 93.97\n",
            "\n",
            "Finding optimal batch_size\n",
            "bs: 1 Error in training\n",
            "bs: 10 Error in training\n",
            "bs: 100 Error in training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aaeewBe5kK9",
        "outputId": "7155b83f-fffc-428e-f0a3-8ef32a5448ae"
      },
      "source": [
        "print(np.max(results))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "93.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qya0gf6MIpe_"
      },
      "source": [
        "#Using Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zA4_AUsnIte7"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "class Layer():\n",
        "    def __init__(self, input_unit, output_unit, act_func):\n",
        "        self.weight = tf.Variable(tf.random.normal([output_unit, input_unit]) * 0.1)\n",
        "        self.bias = tf.Variable(tf.random.normal([output_unit, 1]) * 0.1)\n",
        "        self.Variables = [self.weight, self.bias]\n",
        "        self.act_func = act_func\n",
        "        self.result = None\n",
        "    def linear_func(self, input):\n",
        "        return tf.matmul(self.weight, input) + self.bias\n",
        "    def forward(self, input):\n",
        "        linear = self.linear_func(input)\n",
        "        if self.act_func == 'softmax':\n",
        "            return tf.nn.softmax(linear)\n",
        "        elif self.act_func == 'relu':\n",
        "            return tf.nn.relu(linear)\n",
        "        elif self.act_func == 'sigmoid':\n",
        "            return tf.sigmoid(linear)\n",
        "        else:\n",
        "            raise ValueError(\"Activation Function Name Error\")\n",
        "\n",
        "class NeuralNetwork():\n",
        "    def __init__(self):\n",
        "        self.Layers = []\n",
        "        self.Variables = []\n",
        "    def forward(self, input):\n",
        "        result = [self.Layers[0].forward(input)]\n",
        "        for layer in self.Layers[1:]:\n",
        "            result.append(layer.forward(result[-1]))\n",
        "        return result[-1]\n",
        "    def addLayer(self, input_unit, output_unit, act_func):\n",
        "        self.Layers.append(Layer(input_unit, output_unit, act_func))\n",
        "        self.Variables.extend(self.Layers[-1].Variables)\n",
        "    def loss_fn(self, input, output):\n",
        "        cost1 = tf.multiply(output, tf.math.log(self.forward(input)))\n",
        "        cost = tf.reduce_sum(cost1, axis=0)\n",
        "        return -tf.reduce_mean(cost)\n",
        "    def grad(self, input, output):\n",
        "        with tf.GradientTape() as tape:\n",
        "            cost = self.loss_fn(input, output)\n",
        "            return tape.gradient(cost, self.Variables)\n",
        "    def train(self, input_, output_, learning_rate, EPOCHS, batch_size):\n",
        "        optimizer = tf.keras.optimizers.SGD(learning_rate = learning_rate)\n",
        "        for iter in range(EPOCHS + 1):\n",
        "            for batch in range(int(len(input_[0]) / batch_size)):\n",
        "                input = input_[:, batch * batch_size : (batch + 1) * batch_size]\n",
        "                output = output_[:, batch * batch_size : (batch + 1) * batch_size]\n",
        "                grads = self.grad(input, output)\n",
        "                optimizer.apply_gradients(grads_and_vars=zip(grads, self.Variables))\n",
        "                error = self.loss_fn(input, output)\n",
        "            if iter % (EPOCHS / 10) == 0:\n",
        "                print(\"iter: {:4} error: {:10.4f}\".format(iter, error))\n",
        "    def test_accuracy(self, input, output):\n",
        "        prob = np.average((np.argmax(self.forward(input), axis=0) == np.argmax(output, axis=0))) * 100\n",
        "        print(\"Test Accuracy: {:10.4f}\".format(prob))\n",
        "        return prob"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2nh0Mfpa_W4",
        "outputId": "2a45fcef-976b-46f5-e507-2e8d65b6aa04"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf # imported for load MNIST data\n",
        "\n",
        "(images_train, labels_train), (images_test, labels_test) = tf.keras.datasets.mnist.load_data()\n",
        "images_train = np.reshape(images_train / 255.0, (len(images_train), len(images_train[0]) * len(images_train[0][0]))).T\n",
        "images_test = np.reshape(images_test / 255.0, (len(images_test), len(images_test[0]) * len(images_test[0][0]))).T\n",
        "\n",
        "def one_hot_encoding(data):\n",
        "    encoded = np.zeros((len(data), max(data) - min(data) + 1))\n",
        "    for idx in range(len(data)):\n",
        "        encoded[idx][data[idx]] = 1\n",
        "    return encoded.T\n",
        "\n",
        "labels_train, labels_test = one_hot_encoding(labels_train), one_hot_encoding(labels_test)\n",
        "model = NeuralNetwork()\n",
        "model.addLayer(len(images_train), 10, 'relu')\n",
        "model.addLayer(10, 20, 'relu')\n",
        "model.addLayer(20, 30, 'relu')\n",
        "model.addLayer(30, len(labels_train), 'softmax')\n",
        "model.train(images_train, labels_train, 0.01, 100, 100)\n",
        "model.test_accuracy(images_test, labels_test)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter:    0 error:     4.6003\n",
            "iter:   10 error:     2.9146\n",
            "iter:   20 error:     2.7718\n",
            "iter:   30 error:     2.7084\n",
            "iter:   40 error:     2.6837\n",
            "iter:   50 error:     2.6561\n",
            "iter:   60 error:     2.6375\n",
            "iter:   70 error:     2.6219\n",
            "iter:   80 error:     2.6072\n",
            "iter:   90 error:     2.5995\n",
            "iter:  100 error:     2.5910\n",
            "Test Accuracy:    92.9100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "92.91"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFh2hiryeloX"
      },
      "source": [
        "# with no batch\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "class Layer():\n",
        "    def __init__(self, input_unit, output_unit, act_func):\n",
        "        self.weight = tf.Variable(tf.random.normal([output_unit, input_unit]) * 0.1)\n",
        "        self.bias = tf.Variable(tf.random.normal([output_unit, 1]) * 0.1)\n",
        "        self.Variables = [self.weight, self.bias]\n",
        "        self.act_func = act_func\n",
        "        self.result = None\n",
        "    def linear_func(self, input):\n",
        "        return tf.matmul(self.weight, input) + self.bias\n",
        "    def forward(self, input):\n",
        "        linear = self.linear_func(input)\n",
        "        if self.act_func == 'softmax':\n",
        "            return tf.nn.softmax(linear)\n",
        "        elif self.act_func == 'relu':\n",
        "            return tf.nn.relu(linear)\n",
        "        elif self.act_func == 'sigmoid':\n",
        "            return tf.sigmoid(linear)\n",
        "        else:\n",
        "            raise ValueError(\"Activation Function Name Error\")\n",
        "\n",
        "class NeuralNetwork():\n",
        "    def __init__(self):\n",
        "        self.Layers = []\n",
        "        self.Variables = []\n",
        "    def forward(self, input):\n",
        "        result = [self.Layers[0].forward(input)]\n",
        "        for layer in self.Layers[1:]:\n",
        "            result.append(layer.forward(result[-1]))\n",
        "        return result[-1]\n",
        "    def addLayer(self, input_unit, output_unit, act_func):\n",
        "        self.Layers.append(Layer(input_unit, output_unit, act_func))\n",
        "        self.Variables.extend(self.Layers[-1].Variables)\n",
        "    def loss_fn(self, input, output):\n",
        "        cost1 = tf.multiply(output, tf.math.log(self.forward(input)))\n",
        "        cost = tf.reduce_sum(cost1, axis=0)\n",
        "        return -tf.reduce_mean(cost)\n",
        "    def grad(self, input, output):\n",
        "        with tf.GradientTape() as tape:\n",
        "            cost = self.loss_fn(input, output)\n",
        "            return tape.gradient(cost, self.Variables)\n",
        "    def train(self, input, output, learning_rate, EPOCHS):\n",
        "        optimizer = tf.keras.optimizers.SGD(learning_rate = learning_rate)\n",
        "        for iter in range(EPOCHS + 1):\n",
        "            grads = self.grad(input, output)\n",
        "            optimizer.apply_gradients(grads_and_vars=zip(grads, self.Variables))\n",
        "            error = self.loss_fn(input, output)\n",
        "            if iter % (EPOCHS / 10) == 0:\n",
        "                print(\"iter: {:4} error: {:10.4f}\".format(iter, error))\n",
        "    def test_accuracy(self, input, output):\n",
        "        prob = np.average((np.argmax(self.forward(input), axis=0) == np.argmax(output, axis=0))) * 100\n",
        "        print(\"Test Accuracy: {:10.4f}\".format(prob))\n",
        "        return prob"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9E9BrDyhAkY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}